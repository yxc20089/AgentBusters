# CIO-Agent FAB++ Environment Configuration
# Copy this file to .env and fill in your values
# See docs/BENCHMARK_GUIDE.md for detailed usage instructions

# LLM Provider Configuration
# Options: openai, anthropic
LLM_PROVIDER=openai

# ============================================
# Option 1: Local vLLM (GPU Server - Recommended)
# ============================================
# Start vLLM server first, then uncomment below:
# OPENAI_API_KEY=dummy
# OPENAI_API_BASE=http://localhost:8000/v1
# OPENAI_BASE_URL=http://localhost:8000/v1

# === ‚ö° B200 È°∂ÈÖç (ÂçïÂç° 192GB HBM3e) ===
# --- Qwen3-235B-A22B (ÂçïÂç° B200!) ---
# python scripts/deploy_vllm.py --model qwen3-235b-b200
# ÊàñÊâãÂä®: vllm serve Qwen/Qwen3-235B-A22B --port 8000 --tensor-parallel-size 1 --max-model-len 65536 --trust-remote-code
# LLM_MODEL=Qwen/Qwen3-235B-A22B

# --- DeepSeek-V3.2-671B (3x B200) ---
# python scripts/deploy_vllm.py --model deepseek-v3-b200
# ÊàñÊâãÂä®: vllm serve deepseek-ai/DeepSeek-V3 --port 8000 --tensor-parallel-size 3 --max-model-len 65536 --trust-remote-code
# LLM_MODEL=deepseek-ai/DeepSeek-V3

# === üöÄ H100 È°∂ÈÖç (8x 80GB) ===
# --- DeepSeek-V3.2-671B FP8 (vLLM) ---
# vllm serve deepseek-ai/DeepSeek-V3 --port 8000 --tensor-parallel-size 8 --dtype float8_e4m3fn --quantization fp8 --trust-remote-code
# LLM_MODEL=deepseek-ai/DeepSeek-V3

# --- DeepSeek-V3.2-NVFP4 (TRT-LLM Êé®ËçêÔºåÊúÄÈ´òÂêûÂêê) ---
# python scripts/deploy_trtllm.py --model deepseek-v3-nvfp4
# Êàñ‰ΩøÁî® Docker Compose: docker-compose -f docker-compose.trtllm.yml up
# LLM_MODEL=deepseek-v3.2-nvfp4

# --- Qwen3-235B-A22B MoE (È°∂Á∫ß MoE) ---
# vllm serve Qwen/Qwen3-235B-A22B --port 8000 --tensor-parallel-size 8 --trust-remote-code
# LLM_MODEL=Qwen/Qwen3-235B-A22B

# === ‰∏ªË¶ÅÁõÆÊ†áÊ®°Âûã ===
# --- Qwen3-32B (Recommended: balance of performance & resources) ---
# vllm serve Qwen/Qwen3-32B --port 8000 --tensor-parallel-size 1  # A100 80GB
# vllm serve Qwen/Qwen3-32B --port 8000 --tensor-parallel-size 2  # 2x A100 40GB
# LLM_MODEL=Qwen/Qwen3-32B

# --- DeepSeek-V3.2 (High performance, requires multi-GPU) ---
# vllm serve deepseek-ai/DeepSeek-V3 --port 8000 --tensor-parallel-size 4 --trust-remote-code
# LLM_MODEL=deepseek-ai/DeepSeek-V3

# --- Qwen3-14B (Lightweight, single GPU) ---
# vllm serve Qwen/Qwen3-14B --port 8000 --tensor-parallel-size 1
# LLM_MODEL=Qwen/Qwen3-14B

# --- Other models ---
# LLM_MODEL=meta-llama/Llama-3.1-70B-Instruct
# LLM_MODEL=mistralai/Mixtral-8x22B-Instruct-v0.1
# LLM_MODEL=deepseek-ai/DeepSeek-R1

# ============================================
# Option 2: OpenRouter API (No local GPU needed)
# ============================================
# Get API key from https://openrouter.ai/keys
# OPENAI_API_KEY=sk-or-v1-xxxxxxxxxxxxx
# OPENAI_API_BASE=https://openrouter.ai/api/v1
# LLM_MODEL=qwen/qwen3-32b
# LLM_MODEL=deepseek/deepseek-chat

# ============================================
# Option 3: OpenAI API
# ============================================
OPENAI_API_KEY=sk-...
LLM_MODEL=gpt-4o

# ============================================
# Option 4: Anthropic API
# ============================================
# LLM_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-...
# LLM_MODEL=claude-sonnet-4-20250514

# ============================================
# Option 5: Google Gemini API
# ============================================
# Method 1: Via OpenRouter (Recommended, easiest)
# OPENAI_API_KEY=sk-or-v1-xxxxxxxxxxxxx
# OPENAI_API_BASE=https://openrouter.ai/api/v1
# LLM_MODEL=google/gemini-2.0-flash-exp:free
# LLM_MODEL=google/gemini-pro-1.5

# Method 2: Direct Google AI Studio API
# Get API key from https://aistudio.google.com/apikey
# OPENAI_API_KEY=your-google-ai-studio-api-key
# OPENAI_API_BASE=https://generativelanguage.googleapis.com/v1beta/openai/
# LLM_MODEL=gemini-2.0-flash-exp
# LLM_MODEL=gemini-1.5-pro

# Purple Agent LLM Temperature
# Set to 0.0 for reproducible benchmark results (deterministic outputs)
# Higher values (e.g., 0.3) allow more creative responses but reduce reproducibility
PURPLE_LLM_TEMPERATURE=0.0

# Purple Agent max tool calls per task
# Maximum number of tool invocations before giving up (default: 25)
PURPLE_MAX_TOOL_CALLS=25

# Purple Agent reasoning/thinking mode (for o1/o3/gpt-5 style models)
# Options: off, low, medium, high (default: high)
# Set to "off" to disable extended thinking
PURPLE_REASONING_EFFORT=high

# Database Configuration (for persistent task storage)
# SQLite (local dev): use separate files to avoid lock conflicts
DATABASE_URL=sqlite+aiosqlite:///tasks.db
PURPLE_DATABASE_URL=sqlite+aiosqlite:///purple_tasks.db
# PostgreSQL (production): can use same DB since it supports concurrent writes
# DATABASE_URL=postgresql+asyncpg://user:pass@localhost/agentbusters
# PURPLE_DATABASE_URL=postgresql+asyncpg://user:pass@localhost/agentbusters

# MCP Server Configuration
# Purple Agent can use remote MCP servers (via HTTP) or in-process MCP (if URLs unset).
# RECOMMENDED: Use in-process MCP (faster, simpler, no external processes needed)
# Just leave these commented out:
# MCP_EDGAR_URL=http://localhost:8101
# MCP_YFINANCE_URL=http://localhost:8102
# MCP_SANDBOX_URL=http://localhost:8103

# Evaluation Settings
# LLM-as-judge for dataset evaluators (bizfinbench/prbench/gdpval)
# 
# For separate evaluator API (e.g., use OpenAI for LLM-as-judge while Purple Agent uses vLLM):
# OPENAI_EVAL_API_KEY=sk-...           # OpenAI API key for evaluator (priority over OPENAI_API_KEY)
# OPENAI_EVAL_API_BASE=https://api.openai.com/v1  # Optional: defaults to OpenAI
#
# Or configure in eval_*.yaml:
#   llm_eval:
#     enabled: true
#     model: gpt-4o-mini
#     api_base: https://api.openai.com/v1
#     temperature: 0.0
#
# EVAL_USE_LLM=true
# EVAL_LLM_MODEL=gpt-4o-mini
# EVAL_LLM_TEMPERATURE=0.0

# Private Evaluation Data Repository (for hidden crypto scenarios)
# EVAL_DATA_REPO=yxc20089/agentbusters-eval-data
# EVAL_DATA_PAT=ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Enable temporal locking (prevents look-ahead bias)
TEMPORAL_LOCK_ENABLED=true

# Noise injection ratio for distractor documents (0.0 to 1.0)
NOISE_INJECTION_RATIO=0.3

# Enable token cost tracking
TOKEN_TRACKING_ENABLED=true

# Default simulation date (YYYY-MM-DD)
# Fixed date to fix the order of the questions
SIMULATION_DATE=2026-02-04

# Enable adversarial debate phase (default: false)
DEBATE_ENABLED=false

# Enable Alpha Score calculation
ALPHA_SCORE_ENABLED=true

# Logging level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Record detailed interactions during crypto benchmark evaluation
# Set to "1" to save Green/Purple Agent message exchanges to results
# Useful for debugging trading decisions
RECORD_INTERACTIONS=0

# AlphaVantage API Key (get from https://www.alphavantage.co/support/#api-key)
# Required for synthetic benchmark generation
ALPHAVANTAGE_API_KEY=your_api_key_here
