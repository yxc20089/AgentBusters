# CIO-Agent FAB++ Environment Configuration
# Copy this file to .env and fill in your values
# See docs/BENCHMARK_GUIDE.md for detailed usage instructions

# LLM Provider Configuration
# Options: openai, anthropic
LLM_PROVIDER=openai

# ============================================
# Option 1: Local vLLM (GPU Server - Recommended)
# ============================================
# Start vLLM server first, then uncomment below:
# OPENAI_API_KEY=dummy
# OPENAI_API_BASE=http://localhost:8000/v1
# OPENAI_BASE_URL=http://localhost:8000/v1

# === ‚ö° B200 È°∂ÈÖç (ÂçïÂç° 192GB HBM3e) ===
# --- Qwen3-235B-A22B (ÂçïÂç° B200!) ---
# python scripts/deploy_vllm.py --model qwen3-235b-b200
# ÊàñÊâãÂä®: vllm serve Qwen/Qwen3-235B-A22B --port 8000 --tensor-parallel-size 1 --max-model-len 65536 --trust-remote-code
# LLM_MODEL=Qwen/Qwen3-235B-A22B

# --- DeepSeek-V3.2-671B (3x B200) ---
# python scripts/deploy_vllm.py --model deepseek-v3-b200
# ÊàñÊâãÂä®: vllm serve deepseek-ai/DeepSeek-V3 --port 8000 --tensor-parallel-size 3 --max-model-len 65536 --trust-remote-code
# LLM_MODEL=deepseek-ai/DeepSeek-V3

# === üöÄ H100 È°∂ÈÖç (8x 80GB) ===
# --- DeepSeek-V3.2-671B FP8 (ÊúÄÂº∫ÊÄßËÉΩ) ---
# vllm serve deepseek-ai/DeepSeek-V3 --port 8000 --tensor-parallel-size 8 --dtype float8_e4m3fn --quantization fp8 --trust-remote-code
# LLM_MODEL=deepseek-ai/DeepSeek-V3

# --- Qwen3-235B-A22B MoE (È°∂Á∫ß MoE) ---
# vllm serve Qwen/Qwen3-235B-A22B --port 8000 --tensor-parallel-size 8 --trust-remote-code
# LLM_MODEL=Qwen/Qwen3-235B-A22B

# === ‰∏ªË¶ÅÁõÆÊ†áÊ®°Âûã ===
# --- Qwen3-32B (Recommended: balance of performance & resources) ---
# vllm serve Qwen/Qwen3-32B --port 8000 --tensor-parallel-size 1  # A100 80GB
# vllm serve Qwen/Qwen3-32B --port 8000 --tensor-parallel-size 2  # 2x A100 40GB
# LLM_MODEL=Qwen/Qwen3-32B

# --- DeepSeek-V3.2 (High performance, requires multi-GPU) ---
# vllm serve deepseek-ai/DeepSeek-V3 --port 8000 --tensor-parallel-size 4 --trust-remote-code
# LLM_MODEL=deepseek-ai/DeepSeek-V3

# --- Qwen3-14B (Lightweight, single GPU) ---
# vllm serve Qwen/Qwen3-14B --port 8000 --tensor-parallel-size 1
# LLM_MODEL=Qwen/Qwen3-14B

# --- Other models ---
# LLM_MODEL=meta-llama/Llama-3.1-70B-Instruct
# LLM_MODEL=mistralai/Mixtral-8x22B-Instruct-v0.1
# LLM_MODEL=deepseek-ai/DeepSeek-R1

# ============================================
# Option 2: OpenRouter API (No local GPU needed)
# ============================================
# Get API key from https://openrouter.ai/keys
# OPENAI_API_KEY=sk-or-v1-xxxxxxxxxxxxx
# OPENAI_API_BASE=https://openrouter.ai/api/v1
# LLM_MODEL=qwen/qwen3-32b
# LLM_MODEL=deepseek/deepseek-chat

# ============================================
# Option 3: OpenAI API
# ============================================
OPENAI_API_KEY=sk-...
LLM_MODEL=gpt-4o

# ============================================
# Option 4: Anthropic API
# ============================================
# LLM_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-...
# LLM_MODEL=claude-sonnet-4-20250514

# Purple Agent LLM Temperature
# Set to 0.0 for reproducible benchmark results (deterministic outputs)
# Higher values (e.g., 0.3) allow more creative responses but reduce reproducibility
PURPLE_LLM_TEMPERATURE=0.0

# Database Configuration (for persistent task storage)
# SQLite (local dev): use separate files to avoid lock conflicts
DATABASE_URL=sqlite+aiosqlite:///tasks.db
PURPLE_DATABASE_URL=sqlite+aiosqlite:///purple_tasks.db
# PostgreSQL (production): can use same DB since it supports concurrent writes
# DATABASE_URL=postgresql+asyncpg://user:pass@localhost/agentbusters
# PURPLE_DATABASE_URL=postgresql+asyncpg://user:pass@localhost/agentbusters

# MCP Server Configuration
# Purple Agent can use remote MCP servers (via HTTP) or in-process MCP (if URLs unset).
# RECOMMENDED: Use in-process MCP (faster, simpler, no external processes needed)
# Just leave these commented out:
# MCP_EDGAR_URL=http://localhost:8101
# MCP_YFINANCE_URL=http://localhost:8102
# MCP_SANDBOX_URL=http://localhost:8103

# Evaluation Settings
# LLM-as-judge for dataset evaluators (bizfinbench/public_csv)
# EVAL_USE_LLM=true
# EVAL_LLM_MODEL=gpt-4o-mini
# EVAL_LLM_TEMPERATURE=0.0

# Private Evaluation Data Repository (for hidden crypto scenarios)
# EVAL_DATA_REPO=yxc20089/agentbusters-eval-data
# EVAL_DATA_PAT=ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Enable temporal locking (prevents look-ahead bias)
TEMPORAL_LOCK_ENABLED=true

# Noise injection ratio for distractor documents (0.0 to 1.0)
NOISE_INJECTION_RATIO=0.3

# Enable token cost tracking
TOKEN_TRACKING_ENABLED=true

# Default simulation date (YYYY-MM-DD)
# If not set, defaults to 1 year before current date
# SIMULATION_DATE=2023-01-01

# Enable adversarial debate phase
DEBATE_ENABLED=true

# Enable Alpha Score calculation
ALPHA_SCORE_ENABLED=true

# Logging level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# AlphaVantage API Key (get from https://www.alphavantage.co/support/#api-key)
# Required for synthetic benchmark generation
ALPHAVANTAGE_API_KEY=your_api_key_here
