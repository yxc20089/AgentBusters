# TensorRT-LLM DeepSeek-V3.2-NVFP4 Docker Compose
# 
# 用于快速部署 DeepSeek-V3.2-NVFP4 的 OpenAI 兼容 API
#
# Usage:
#   docker-compose -f docker-compose.trtllm.yml up
#
# 验证:
#   curl http://localhost:8000/v1/models
#   curl http://localhost:8000/v1/chat/completions \
#     -H "Content-Type: application/json" \
#     -d '{"model": "deepseek-v3.2-nvfp4", "messages": [{"role": "user", "content": "Hello!"}]}'

version: '3.8'

services:
  trtllm-api:
    image: nvcr.io/nvidia/tensorrt-llm/release:1.3.0rc1
    container_name: deepseek-v3-nvfp4
    
    # GPU 配置 - 需要 8x H100/A100 80GB
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    # 端口映射
    ports:
      - "8000:8000"
    
    # 挂载卷
    volumes:
      # API 代码
      - ./src/trtllm_api:/app
      # 模型缓存 (HuggingFace 会自动下载)
      - ${HF_HOME:-~/.cache/huggingface}:/root/.cache/huggingface
      # 可选: 本地模型路径
      # - /mnt/models:/models
    
    # 工作目录
    working_dir: /app
    
    # 环境变量
    environment:
      - MODEL=${TRTLLM_MODEL:-nvidia/DeepSeek-V3.2-NVFP4}
      - TENSOR_PARALLEL_SIZE=${TENSOR_PARALLEL_SIZE:-8}
      - PORT=8000
      - HF_TOKEN=${HF_TOKEN:-}
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}
    
    # 启动命令
    command: bash /app/start.sh
    
    # 共享内存配置 (TRT-LLM 需要)
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    
    # 健康检查
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s  # 模型加载需要时间

    # 日志配置
    logging:
      driver: json-file
      options:
        max-size: "100m"
        max-file: "3"

    # 重启策略
    restart: unless-stopped


# 可选: 如果需要单独的 HuggingFace 模型下载服务
# services:
#   model-downloader:
#     image: huggingface/downloader
#     volumes:
#       - ${HF_HOME:-~/.cache/huggingface}:/root/.cache/huggingface
#     command: huggingface-cli download nvidia/DeepSeek-V3.2-NVFP4
