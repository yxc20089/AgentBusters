#!/usr/bin/env python3
"""
TensorRT-LLM æ¨¡å‹éƒ¨ç½²è„šæœ¬

ç”¨äºéƒ¨ç½² DeepSeek-V3.2-NVFP4 ç­‰ NVIDIA é¢„é‡åŒ–æ¨¡å‹ã€‚
TRT-LLM + NVFP4 æ˜¯ NVIDIA å®˜æ–¹æ¨èçš„ DeepSeek-V3 éƒ¨ç½²è·¯çº¿ã€‚

Usage:
    # åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹é…ç½®
    python scripts/deploy_trtllm.py --list
    
    # éƒ¨ç½² DeepSeek-V3.2-NVFP4 (8x H100)
    python scripts/deploy_trtllm.py --model deepseek-v3-nvfp4
    
    # è‡ªå®šä¹‰ç«¯å£
    python scripts/deploy_trtllm.py --model deepseek-v3-nvfp4 --port 8001
    
    # ç”Ÿæˆ Docker å‘½ä»¤ä½†ä¸æ‰§è¡Œ
    python scripts/deploy_trtllm.py --model deepseek-v3-nvfp4 --dry-run

Why TRT-LLM + NVFP4?
    - NVFP4 æ˜¯é¢„é‡åŒ–æ¨¡å‹ï¼Œä¸éœ€è¦ build engine
    - æ¯” vLLM æ›´é«˜ååé‡
    - 8x H100 80GB ç¨³å®šè¿è¡Œ 671B MoE
    - NVIDIA å®˜æ–¹æ”¯æŒçš„ DeepSeek-V3 è·¯çº¿
"""

import argparse
import os
import subprocess
import sys
from pathlib import Path
from typing import Optional


# TRT-LLM æ¨¡å‹é…ç½®
TRTLLM_MODEL_CONFIGS = {
    # ========== DeepSeek-V3.2 NVFP4 (æ¨è) ==========
    "deepseek-v3-nvfp4": {
        "name": "DeepSeek-V3.2-NVFP4 (TRT-LLM)",
        "hf_model": "nvidia/DeepSeek-V3.2-NVFP4",
        "description": "NVIDIA å®˜æ–¹é¢„é‡åŒ– FP4ï¼Œ8x H100 80GBï¼Œæœ€é«˜åå",
        "min_gpus": 8,
        "recommended_gpus": 8,
        "gpu_memory": "8x80GB (H100/A100)",
        "default_max_len": 32768,
        "tensor_parallel_size": 8,
        "container_image": "nvcr.io/nvidia/tensorrt-llm/release:1.3.0rc1",
        "trust_remote_code": True,
        "env_model_name": "deepseek-v3.2-nvfp4",
    },
    "deepseek-v3-nvfp4-4gpu": {
        "name": "DeepSeek-V3.2-NVFP4 (4x GPU)",
        "hf_model": "nvidia/DeepSeek-V3.2-NVFP4",
        "description": "NVIDIA é¢„é‡åŒ– FP4ï¼Œ4x A100 80GB (å®éªŒæ€§)",
        "min_gpus": 4,
        "recommended_gpus": 4,
        "gpu_memory": "4x80GB",
        "default_max_len": 16384,
        "tensor_parallel_size": 4,
        "container_image": "nvcr.io/nvidia/tensorrt-llm/release:1.3.0rc1",
        "trust_remote_code": True,
        "env_model_name": "deepseek-v3.2-nvfp4",
    },
}


def list_models():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ TRT-LLM æ¨¡å‹é…ç½®"""
    print("\n" + "=" * 70)
    print("TensorRT-LLM å¯ç”¨æ¨¡å‹é…ç½®")
    print("=" * 70)
    
    print("\nğŸš€ NVIDIA é¢„é‡åŒ–æ¨¡å‹ (æ¨è):")
    print("-" * 70)
    
    for key, cfg in TRTLLM_MODEL_CONFIGS.items():
        print(f"\n  {key}")
        print(f"    æ¨¡å‹: {cfg['name']}")
        print(f"    æè¿°: {cfg['description']}")
        print(f"    GPU: {cfg['min_gpus']}x ({cfg['gpu_memory']})")
        print(f"    HuggingFace: {cfg['hf_model']}")
        print(f"    å®¹å™¨: {cfg['container_image']}")
    
    print("\n" + "=" * 70)
    print("éƒ¨ç½²æ¶æ„:")
    print("-" * 70)
    print("""
    Client (OpenAI SDK / curl)
            |
            v
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  OpenAI-compatible API  â”‚  â† FastAPI (trtllm_openai_api.py)
    â”‚  (adapter / gateway)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               |
               v
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   TensorRT-LLM Runtime  â”‚
    â”‚   (DeepSeek-V3.2-NVFP4) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               |
               v
           8Ã—H100 / A100
    """)
    print("=" * 70)


def build_docker_command(
    model_key: str,
    port: int = 8000,
    model_path: str = "/mnt/models",
) -> list:
    """æ„å»º Docker å¯åŠ¨å‘½ä»¤"""
    
    if model_key not in TRTLLM_MODEL_CONFIGS:
        print(f"Error: æœªçŸ¥æ¨¡å‹ '{model_key}'")
        print(f"å¯ç”¨æ¨¡å‹: {', '.join(TRTLLM_MODEL_CONFIGS.keys())}")
        sys.exit(1)
    
    cfg = TRTLLM_MODEL_CONFIGS[model_key]
    
    cmd = [
        "docker", "run", "--rm", "-it",
        "--gpus", "all",
        "--ipc=host",
        "--ulimit", "memlock=-1",
        "--ulimit", "stack=67108864",
        "-p", f"{port}:8000",
        "-v", f"{model_path}:/models",
        "-v", f"{Path(__file__).parent.parent / 'src' / 'trtllm_api'}:/app",
        "-w", "/app",
        cfg["container_image"],
        "python", "trtllm_openai_api.py",
        "--model", cfg["hf_model"],
        "--tensor-parallel-size", str(cfg["tensor_parallel_size"]),
    ]
    
    return cmd


def generate_api_start_script(model_key: str, port: int = 8000) -> str:
    """ç”Ÿæˆå®¹å™¨å†…è¿è¡Œçš„å¯åŠ¨è„šæœ¬"""
    
    cfg = TRTLLM_MODEL_CONFIGS[model_key]
    
    script = f'''#!/bin/bash
# TRT-LLM OpenAI API å¯åŠ¨è„šæœ¬
# åœ¨ TRT-LLM å®¹å™¨å†…è¿è¡Œ

cd /app
pip install fastapi uvicorn pydantic

python trtllm_openai_api.py \\
    --model {cfg["hf_model"]} \\
    --tensor-parallel-size {cfg["tensor_parallel_size"]} \\
    --port {port}
'''
    return script


def update_env_file(model_key: str, port: int = 8000):
    """æ›´æ–° .env æ–‡ä»¶ä¸­çš„æ¨¡å‹é…ç½®"""
    
    cfg = TRTLLM_MODEL_CONFIGS[model_key]
    env_path = Path(__file__).parent.parent / ".env"
    
    if not env_path.exists():
        print("Error: .env æ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    content = env_path.read_text()
    lines = content.split("\n")
    new_lines = []
    updated = False
    
    for line in lines:
        stripped = line.strip()
        
        # æ›´æ–° OPENAI_API_BASE
        if stripped.startswith("OPENAI_API_BASE=") and not stripped.startswith("#"):
            new_lines.append(f"OPENAI_API_BASE=http://localhost:{port}/v1")
            updated = True
            continue
        
        # æ›´æ–° LLM_MODEL
        if stripped.startswith("LLM_MODEL=") and not stripped.startswith("#"):
            new_lines.append(f"LLM_MODEL={cfg['env_model_name']}")
            continue
        
        # æ›´æ–° OPENAI_API_KEY (for local TRT-LLM)
        if stripped.startswith("OPENAI_API_KEY=") and not stripped.startswith("#"):
            new_lines.append("OPENAI_API_KEY=dummy")
            continue
        
        new_lines.append(line)
    
    env_path.write_text("\n".join(new_lines))
    
    if updated:
        print(f"âœ… å·²æ›´æ–° .env æ–‡ä»¶:")
        print(f"   OPENAI_API_BASE=http://localhost:{port}/v1")
        print(f"   LLM_MODEL={cfg['env_model_name']}")
        print(f"   OPENAI_API_KEY=dummy")
    
    return True


def main():
    parser = argparse.ArgumentParser(
        description="TensorRT-LLM æ¨¡å‹éƒ¨ç½²è„šæœ¬ (DeepSeek-V3.2-NVFP4)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    parser.add_argument(
        "--model", "-m",
        choices=list(TRTLLM_MODEL_CONFIGS.keys()),
        help="è¦éƒ¨ç½²çš„æ¨¡å‹",
    )
    parser.add_argument(
        "--list", "-l",
        action="store_true",
        help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹",
    )
    parser.add_argument(
        "--port", "-p",
        type=int,
        default=8000,
        help="API æœåŠ¡ç«¯å£ (default: 8000)",
    )
    parser.add_argument(
        "--model-path",
        type=str,
        default="/mnt/models",
        help="æ¨¡å‹å­˜å‚¨è·¯å¾„ (default: /mnt/models)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="åªæ‰“å°å‘½ä»¤ï¼Œä¸æ‰§è¡Œ",
    )
    parser.add_argument(
        "--update-env",
        action="store_true",
        help="åŒæ—¶æ›´æ–° .env æ–‡ä»¶",
    )
    parser.add_argument(
        "--generate-script",
        action="store_true",
        help="ç”Ÿæˆå®¹å™¨å†…å¯åŠ¨è„šæœ¬",
    )
    
    args = parser.parse_args()
    
    # åˆ—å‡ºæ¨¡å‹
    if args.list:
        list_models()
        return
    
    # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†æ¨¡å‹
    if not args.model:
        print("Error: è¯·æŒ‡å®šè¦éƒ¨ç½²çš„æ¨¡å‹ (--model)")
        print("ä½¿ç”¨ --list æŸ¥çœ‹å¯ç”¨æ¨¡å‹")
        sys.exit(1)
    
    cfg = TRTLLM_MODEL_CONFIGS[args.model]
    
    print("\n" + "=" * 70)
    print(f"éƒ¨ç½²æ¨¡å‹: {cfg['name']}")
    print("=" * 70)
    print(f"\næè¿°: {cfg['description']}")
    print(f"HuggingFace: {cfg['hf_model']}")
    print(f"ç«¯å£: {args.port}")
    print(f"å®¹å™¨: {cfg['container_image']}")
    
    # ç”Ÿæˆå¯åŠ¨è„šæœ¬
    if args.generate_script:
        script = generate_api_start_script(args.model, args.port)
        script_path = Path(__file__).parent.parent / "src" / "trtllm_api" / "start.sh"
        script_path.parent.mkdir(parents=True, exist_ok=True)
        script_path.write_text(script)
        print(f"\nâœ… å·²ç”Ÿæˆå¯åŠ¨è„šæœ¬: {script_path}")
        return
    
    # æ„å»º Docker å‘½ä»¤
    cmd = build_docker_command(
        model_key=args.model,
        port=args.port,
        model_path=args.model_path,
    )
    
    print(f"\nDocker å‘½ä»¤:")
    print(f"  {' '.join(cmd)}")
    
    # æ›´æ–° .env
    if args.update_env:
        print("\n")
        update_env_file(args.model, args.port)
    
    # æ‰§è¡Œ
    if args.dry_run:
        print("\n[Dry run - ä¸æ‰§è¡Œå‘½ä»¤]")
        print("\nğŸ“Œ æ‰‹åŠ¨å¯åŠ¨æ­¥éª¤:")
        print("1. æ‹‰å– TRT-LLM å®¹å™¨:")
        print(f"   docker pull {cfg['container_image']}")
        print("\n2. å¯åŠ¨å®¹å™¨:")
        print(f"   {' '.join(cmd)}")
        print("\n3. åœ¨å®¹å™¨å†…å¯åŠ¨ API:")
        print("   cd /app && python trtllm_openai_api.py")
    else:
        print("\nå¯åŠ¨ TRT-LLM æœåŠ¡...")
        print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡\n")
        try:
            subprocess.run(cmd)
        except KeyboardInterrupt:
            print("\n\næœåŠ¡å·²åœæ­¢")
        except FileNotFoundError:
            print("\nError: Docker æœªå®‰è£…æˆ–æœªè¿è¡Œ")
            sys.exit(1)


if __name__ == "__main__":
    main()
