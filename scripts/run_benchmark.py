#!/usr/bin/env python3
"""
AgentBusters Benchmark ä¸€é”®è¿è¡Œè„šæœ¬

å®Œæ•´æµç¨‹ï¼šå¯åŠ¨ agents -> è¿è¡Œè¯„æµ‹ -> æ”¶é›†ç»“æœ

Usage:
    # å¿«é€Ÿæµ‹è¯• (10 tasks)
    python scripts/run_benchmark.py --quick
    
    # ä¸­ç­‰è§„æ¨¡è¯„æµ‹ (100 tasks)
    python scripts/run_benchmark.py --model qwen3-32b --tasks 100
    
    # å¤§è§„æ¨¡è¯„æµ‹ (500 tasks)
    python scripts/run_benchmark.py --model qwen3-32b --config eval_large.yaml --tasks 500
    
    # å¯¹æ¯”ä¸‰ä¸ªç›®æ ‡æ¨¡å‹
    python scripts/run_benchmark.py --compare-models
    
    # åªå¯åŠ¨ agents (ä¸è¿è¡Œè¯„æµ‹)
    python scripts/run_benchmark.py --model qwen3-32b --start-only

Examples:
    # ä½¿ç”¨æœ¬åœ° vLLM (å‡è®¾å·²å¯åŠ¨)
    python scripts/run_benchmark.py --model qwen3-32b --tasks 100 --vllm-url http://localhost:8000/v1
    
    # ä½¿ç”¨ OpenRouter API
    python scripts/run_benchmark.py --model qwen3-32b --tasks 100 --api openrouter

æ³¨æ„ï¼šè¿è¡Œå‰è¯·ç¡®ä¿ï¼š
1. vLLM æœåŠ¡å·²å¯åŠ¨ (æˆ–ä½¿ç”¨ --api openrouter)
2. .env æ–‡ä»¶å·²æ­£ç¡®é…ç½®
"""

import argparse
import asyncio
import json
import os
import signal
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Optional

# æ¨¡å‹é…ç½®
MODELS = {
    # B200 é¡¶é… (192GB HBM3e)
    "qwen3-235b-b200": "Qwen/Qwen3-235B-A22B",   # 1x B200 192GB
    "deepseek-v3-b200": "deepseek-ai/DeepSeek-V3", # 3x B200 192GB
    # H100 é¡¶é… (8x 80GB)
    "deepseek-v3-fp8": "deepseek-ai/DeepSeek-V3",
    "qwen3-235b": "Qwen/Qwen3-235B-A22B",
    # ä¸»è¦ç›®æ ‡æ¨¡å‹
    "qwen3-32b": "Qwen/Qwen3-32B",
    "deepseek-v3": "deepseek-ai/DeepSeek-V3",
    "qwen3-14b": "Qwen/Qwen3-14B",
    # å…¶ä»–
    "llama3.1-70b": "meta-llama/Llama-3.1-70B-Instruct",
}

# è¯„æµ‹é…ç½®
EVAL_CONFIGS = {
    "quick": ("config/eval_quick_test.yaml", 10),
    "medium": ("config/eval_medium.yaml", 100),
    "large": ("config/eval_large.yaml", 500),
}


def check_vllm_health(url: str, timeout: int = 5) -> bool:
    """æ£€æŸ¥ vLLM æœåŠ¡æ˜¯å¦å¯ç”¨"""
    import httpx
    try:
        response = httpx.get(f"{url}/models", timeout=timeout)
        return response.status_code == 200
    except:
        return False


def start_green_agent(
    eval_config: str,
    port: int = 9109,
    host: str = "0.0.0.0",
) -> subprocess.Popen:
    """å¯åŠ¨ Green Agent"""
    cmd = [
        sys.executable,
        "src/cio_agent/a2a_server.py",
        "--host", host,
        "--port", str(port),
        "--eval-config", eval_config,
        "--store-predicted",
        "--predicted-max-chars", "200",
    ]
    
    print(f"ğŸŸ¢ å¯åŠ¨ Green Agent (port {port})...")
    proc = subprocess.Popen(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
        bufsize=1,
    )
    return proc


def start_purple_agent(
    port: int = 9110,
    host: str = "0.0.0.0",
) -> subprocess.Popen:
    """å¯åŠ¨ Purple Agent"""
    cmd = [
        "purple-agent", "serve",
        "--host", host,
        "--port", str(port),
        "--card-url", f"http://127.0.0.1:{port}",
    ]
    
    print(f"ğŸŸ£ å¯åŠ¨ Purple Agent (port {port})...")
    proc = subprocess.Popen(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
        bufsize=1,
    )
    return proc


def wait_for_agent(url: str, name: str, timeout: int = 60) -> bool:
    """ç­‰å¾… agent å¯åŠ¨"""
    import httpx
    
    start = time.time()
    while time.time() - start < timeout:
        try:
            response = httpx.get(f"{url}/.well-known/agent.json", timeout=5)
            if response.status_code == 200:
                print(f"  âœ… {name} å·²å°±ç»ª")
                return True
        except:
            pass
        time.sleep(2)
        print(f"  â³ ç­‰å¾… {name}...")
    
    print(f"  âŒ {name} å¯åŠ¨è¶…æ—¶")
    return False


def run_evaluation(
    green_url: str,
    purple_url: str,
    num_tasks: int,
    output_file: str,
    timeout: int = 3600,
    conduct_debate: bool = False,
) -> dict:
    """è¿è¡Œè¯„æµ‹"""
    cmd = [
        sys.executable,
        "scripts/run_a2a_eval.py",
        "--green-url", green_url,
        "--purple-url", purple_url,
        "--num-tasks", str(num_tasks),
        "--timeout", str(timeout),
        "-v",
        "-o", output_file,
    ]
    
    if conduct_debate:
        cmd.append("--conduct-debate")
    
    print(f"\nğŸ“Š å¼€å§‹è¯„æµ‹ ({num_tasks} tasks)...")
    result = subprocess.run(cmd, capture_output=False)
    
    return {"success": result.returncode == 0, "output_file": output_file}


def update_env_for_model(model_key: str, api_base: str):
    """ä¸´æ—¶æ›´æ–°ç¯å¢ƒå˜é‡"""
    if model_key in MODELS:
        os.environ["LLM_MODEL"] = MODELS[model_key]
    os.environ["OPENAI_API_BASE"] = api_base
    os.environ["OPENAI_BASE_URL"] = api_base
    if "localhost" in api_base or "127.0.0.1" in api_base:
        os.environ["OPENAI_API_KEY"] = "dummy"


def run_single_benchmark(
    model_key: str,
    eval_config: str,
    num_tasks: int,
    vllm_url: str,
    output_dir: str,
    green_port: int = 9109,
    purple_port: int = 9110,
    timeout: int = 3600,
    start_only: bool = False,
) -> dict:
    """è¿è¡Œå•ä¸ªæ¨¡å‹çš„ benchmark"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = Path(output_dir) / f"{model_key}_{timestamp}.json"
    
    print("\n" + "=" * 70)
    print(f"ğŸ“‹ Benchmark: {model_key}")
    print("=" * 70)
    print(f"  æ¨¡å‹: {MODELS.get(model_key, model_key)}")
    print(f"  é…ç½®: {eval_config}")
    print(f"  ä»»åŠ¡æ•°: {num_tasks}")
    print(f"  vLLM: {vllm_url}")
    
    # æ›´æ–°ç¯å¢ƒå˜é‡
    update_env_for_model(model_key, vllm_url)
    
    # æ£€æŸ¥ vLLM
    if not check_vllm_health(vllm_url):
        print(f"\nâš ï¸  vLLM æœåŠ¡ä¸å¯ç”¨: {vllm_url}")
        print("è¯·å…ˆå¯åŠ¨ vLLM æœåŠ¡:")
        print(f"  python scripts/deploy_vllm.py --model {model_key}")
        return {"success": False, "error": "vLLM not available"}
    
    print(f"  âœ… vLLM æœåŠ¡å¯ç”¨")
    
    # å¯åŠ¨ agents
    processes = []
    try:
        green_proc = start_green_agent(eval_config, green_port)
        processes.append(green_proc)
        
        purple_proc = start_purple_agent(purple_port)
        processes.append(purple_proc)
        
        # ç­‰å¾… agents å¯åŠ¨
        green_url = f"http://127.0.0.1:{green_port}"
        purple_url = f"http://127.0.0.1:{purple_port}"
        
        if not wait_for_agent(green_url, "Green Agent"):
            return {"success": False, "error": "Green Agent failed to start"}
        
        if not wait_for_agent(purple_url, "Purple Agent"):
            return {"success": False, "error": "Purple Agent failed to start"}
        
        if start_only:
            print("\nâœ… Agents å·²å¯åŠ¨ï¼ŒæŒ‰ Ctrl+C åœæ­¢")
            try:
                while True:
                    time.sleep(1)
            except KeyboardInterrupt:
                print("\nåœæ­¢ agents...")
            return {"success": True, "mode": "start-only"}
        
        # è¿è¡Œè¯„æµ‹
        result = run_evaluation(
            green_url=green_url,
            purple_url=purple_url,
            num_tasks=num_tasks,
            output_file=str(output_file),
            timeout=timeout,
        )
        
        return result
        
    finally:
        # æ¸…ç†è¿›ç¨‹
        for proc in processes:
            if proc.poll() is None:
                proc.terminate()
                try:
                    proc.wait(timeout=5)
                except:
                    proc.kill()


def compare_models(
    models: list,
    eval_config: str,
    num_tasks: int,
    vllm_url: str,
    output_dir: str,
):
    """å¯¹æ¯”å¤šä¸ªæ¨¡å‹"""
    
    results = []
    
    for model_key in models:
        print(f"\n\n{'#' * 70}")
        print(f"# æ¨¡å‹ {len(results) + 1}/{len(models)}: {model_key}")
        print(f"{'#' * 70}")
        
        result = run_single_benchmark(
            model_key=model_key,
            eval_config=eval_config,
            num_tasks=num_tasks,
            vllm_url=vllm_url,
            output_dir=output_dir,
        )
        
        result["model"] = model_key
        results.append(result)
        
        # æ¨¡å‹ä¹‹é—´çš„é—´éš”
        if model_key != models[-1]:
            print("\nâ³ åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹...")
            print("   è¯·åœ¨å¦ä¸€ä¸ªç»ˆç«¯ä¸­é‡å¯ vLLM æœåŠ¡:")
            next_idx = models.index(model_key) + 1
            print(f"   python scripts/deploy_vllm.py --model {models[next_idx]}")
            input("   å‡†å¤‡å¥½åæŒ‰ Enter ç»§ç»­...")
    
    # ä¿å­˜æ±‡æ€»
    summary_file = Path(output_dir) / "comparison_summary.json"
    with open(summary_file, "w") as f:
        json.dump({
            "timestamp": datetime.now().isoformat(),
            "models": models,
            "eval_config": eval_config,
            "num_tasks": num_tasks,
            "results": results,
        }, f, indent=2)
    
    print(f"\nâœ… å¯¹æ¯”ç»“æœå·²ä¿å­˜: {summary_file}")
    return results


def main():
    parser = argparse.ArgumentParser(
        description="AgentBusters Benchmark ä¸€é”®è¿è¡Œè„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    
    # æ¨¡å‹é€‰æ‹©
    parser.add_argument(
        "--model", "-m",
        choices=list(MODELS.keys()),
        default="qwen3-32b",
        help="è¦è¯„æµ‹çš„æ¨¡å‹ (default: qwen3-32b)",
    )
    parser.add_argument(
        "--compare-models",
        action="store_true",
        help="å¯¹æ¯”ä¸‰ä¸ªç›®æ ‡æ¨¡å‹ (qwen3-32b, deepseek-v3, qwen3-14b)",
    )
    parser.add_argument(
        "--compare-flagship",
        action="store_true",
        help="å¯¹æ¯”é¡¶é…æ¨¡å‹ (deepseek-v3-fp8, qwen3-235b) - éœ€è¦ 8x H100",
    )
    
    # è¯„æµ‹è§„æ¨¡
    parser.add_argument(
        "--quick",
        action="store_true",
        help="å¿«é€Ÿæµ‹è¯•æ¨¡å¼ (10 tasks)",
    )
    parser.add_argument(
        "--tasks", "-n",
        type=int,
        default=100,
        help="ä»»åŠ¡æ•°é‡ (default: 100)",
    )
    parser.add_argument(
        "--config", "-c",
        help="è¯„æµ‹é…ç½®æ–‡ä»¶ (default: config/eval_medium.yaml)",
    )
    
    # æœåŠ¡é…ç½®
    parser.add_argument(
        "--vllm-url",
        default="http://localhost:8000/v1",
        help="vLLM æœåŠ¡åœ°å€",
    )
    parser.add_argument(
        "--green-port",
        type=int,
        default=9109,
        help="Green Agent ç«¯å£",
    )
    parser.add_argument(
        "--purple-port",
        type=int,
        default=9110,
        help="Purple Agent ç«¯å£",
    )
    
    # è¾“å‡º
    parser.add_argument(
        "--output-dir", "-o",
        default="results/benchmarks",
        help="ç»“æœè¾“å‡ºç›®å½•",
    )
    
    # å…¶ä»–é€‰é¡¹
    parser.add_argument(
        "--timeout",
        type=int,
        default=3600,
        help="è¯„æµ‹è¶…æ—¶æ—¶é—´ (ç§’)",
    )
    parser.add_argument(
        "--start-only",
        action="store_true",
        help="åªå¯åŠ¨ agentsï¼Œä¸è¿è¡Œè¯„æµ‹",
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    
    # ç¡®å®šè¯„æµ‹é…ç½®
    if args.quick:
        eval_config, num_tasks = EVAL_CONFIGS["quick"]
    elif args.config:
        eval_config = args.config
        num_tasks = args.tasks
    else:
        eval_config = "config/eval_medium.yaml"
        num_tasks = args.tasks
    
    print("\n" + "=" * 70)
    print("ğŸš€ AgentBusters Benchmark Runner")
    print("=" * 70)
    
    # è¿è¡Œæ¨¡å¼
    if args.compare_flagship:
        compare_models(
            models=["deepseek-v3-fp8", "qwen3-235b"],
            eval_config="config/eval_large.yaml",
            num_tasks=500,
            vllm_url=args.vllm_url,
            output_dir=args.output_dir,
        )
    elif args.compare_models:
        compare_models(
            models=["qwen3-32b", "deepseek-v3", "qwen3-14b"],
            eval_config=eval_config,
            num_tasks=num_tasks,
            vllm_url=args.vllm_url,
            output_dir=args.output_dir,
        )
    else:
        result = run_single_benchmark(
            model_key=args.model,
            eval_config=eval_config,
            num_tasks=num_tasks,
            vllm_url=args.vllm_url,
            output_dir=args.output_dir,
            green_port=args.green_port,
            purple_port=args.purple_port,
            timeout=args.timeout,
            start_only=args.start_only,
        )
        
        if result.get("success"):
            print(f"\nâœ… è¯„æµ‹å®Œæˆ!")
            if "output_file" in result:
                print(f"   ç»“æœæ–‡ä»¶: {result['output_file']}")
        else:
            print(f"\nâŒ è¯„æµ‹å¤±è´¥: {result.get('error', 'Unknown error')}")
            sys.exit(1)


if __name__ == "__main__":
    main()
