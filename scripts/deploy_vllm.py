#!/usr/bin/env python3
"""
vLLM æ¨¡å‹éƒ¨ç½²è„šæœ¬

ç”¨äºå¿«é€Ÿéƒ¨ç½² Qwen3-32Bã€DeepSeek-V3.2ã€Qwen3-14B ç­‰æ¨¡å‹ã€‚

Usage:
    # åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹é…ç½®
    python scripts/deploy_vllm.py --list
    
    # éƒ¨ç½² Qwen3-32B
    python scripts/deploy_vllm.py --model qwen3-32b
    
    # éƒ¨ç½² DeepSeek-V3
    python scripts/deploy_vllm.py --model deepseek-v3 --gpus 4
    
    # éƒ¨ç½² Qwen3-14B (è½»é‡çº§)
    python scripts/deploy_vllm.py --model qwen3-14b
    
    # è‡ªå®šä¹‰ç«¯å£
    python scripts/deploy_vllm.py --model qwen3-32b --port 8001
    
    # ç”Ÿæˆéƒ¨ç½²å‘½ä»¤ä½†ä¸æ‰§è¡Œ
    python scripts/deploy_vllm.py --model qwen3-32b --dry-run

Examples:
    # å¿«é€Ÿå¼€å§‹ï¼šä½¿ç”¨é»˜è®¤é…ç½®éƒ¨ç½² Qwen3-32B
    python scripts/deploy_vllm.py --model qwen3-32b
    
    # ç„¶åæ›´æ–° .env æ–‡ä»¶
    python scripts/deploy_vllm.py --model qwen3-32b --update-env
"""

import argparse
import os
import subprocess
import sys
from pathlib import Path
from typing import Optional


# æ¨¡å‹é…ç½®å®šä¹‰
MODEL_CONFIGS = {
    # ========== B200 é¡¶é… (å•å¡ 192GB HBM3e) ==========
    "qwen3-235b-b200": {
        "name": "Qwen3-235B-A22B (B200 å•å¡)",
        "hf_model": "Qwen/Qwen3-235B-A22B",
        "description": "B200 é¡¶é…ï¼šå•å¡ 192GB è¿è¡Œ 235B MoE",
        "min_gpus": 1,
        "recommended_gpus": 1,
        "gpu_memory": "1x192GB (B200)",
        "default_max_len": 65536,  # B200 å¤§æ˜¾å­˜æ”¯æŒæ›´é•¿ context
        "extra_args": [
            "--trust-remote-code",
        ],
        "env_model_name": "Qwen/Qwen3-235B-A22B",
    },
    "deepseek-v3-b200": {
        "name": "DeepSeek-V3.2-671B (3x B200)",
        "hf_model": "deepseek-ai/DeepSeek-V3",
        "description": "B200 é¡¶é…ï¼š3å¡è¿è¡Œ 671B MoEï¼ŒBF16 å…¨ç²¾åº¦",
        "min_gpus": 3,
        "recommended_gpus": 3,
        "gpu_memory": "3x192GB (B200)",
        "default_max_len": 65536,  # B200 å¤§æ˜¾å­˜æ”¯æŒæ›´é•¿ context
        "extra_args": [
            "--trust-remote-code",
        ],
        "env_model_name": "deepseek-ai/DeepSeek-V3",
    },
    # ========== H100 é¡¶é… (8x 80GB) ==========
    "deepseek-v3-fp8": {
        "name": "DeepSeek-V3.2-671B (FP8)",
        "hf_model": "deepseek-ai/DeepSeek-V3",
        "description": "H100 é¡¶é…ï¼š671B MoEï¼ŒFP8 åŸç”Ÿç²¾åº¦ï¼Œ8x H100 80GB",
        "min_gpus": 8,
        "recommended_gpus": 8,
        "gpu_memory": "8x80GB",
        "default_max_len": 32768,
        "extra_args": [
            "--trust-remote-code",
            "--dtype", "float8_e4m3fn",  # FP8 åŸç”Ÿ
            "--quantization", "fp8",
            "--kv-cache-dtype", "fp8_e4m3",
        ],
        "env_model_name": "deepseek-ai/DeepSeek-V3",
    },
    "qwen3-235b": {
        "name": "Qwen3-235B-A22B (MoE)",
        "hf_model": "Qwen/Qwen3-235B-A22B",
        "description": "H100 é¡¶é…ï¼š235B MoE (22B æ¿€æ´»)ï¼Œ8x H100 80GB",
        "min_gpus": 8,
        "recommended_gpus": 8,
        "gpu_memory": "8x80GB",
        "default_max_len": 32768,
        "extra_args": [
            "--trust-remote-code",
        ],
        "env_model_name": "Qwen/Qwen3-235B-A22B",
    },
    # ========== ä¸»è¦ç›®æ ‡æ¨¡å‹ ==========
    "qwen3-32b": {
        "name": "Qwen3-32B",
        "hf_model": "Qwen/Qwen3-32B",
        "description": "æ¨èæ¨¡å‹ï¼Œå¹³è¡¡æ€§èƒ½ä¸èµ„æºæ¶ˆè€—",
        "min_gpus": 1,
        "recommended_gpus": 1,
        "gpu_memory": "80GB",
        "default_max_len": 32768,
        "extra_args": [],
        "env_model_name": "Qwen/Qwen3-32B",
    },
    "deepseek-v3": {
        "name": "DeepSeek-V3.2",
        "hf_model": "deepseek-ai/DeepSeek-V3",
        "description": "671B MoE æ¨¡å‹ï¼Œéœ€è¦å¤š GPU (FP16/BF16)",
        "min_gpus": 4,
        "recommended_gpus": 8,
        "gpu_memory": "4x80GB+",
        "default_max_len": 16384,
        "extra_args": ["--trust-remote-code"],
        "env_model_name": "deepseek-ai/DeepSeek-V3",
    },
    "qwen3-14b": {
        "name": "Qwen3-14B",
        "hf_model": "Qwen/Qwen3-14B",
        "description": "è½»é‡çº§æ¨¡å‹ï¼Œé€‚åˆå• GPU éƒ¨ç½²",
        "min_gpus": 1,
        "recommended_gpus": 1,
        "gpu_memory": "24GB+",
        "default_max_len": 32768,
        "extra_args": [],
        "env_model_name": "Qwen/Qwen3-14B",
    },
    # ========== å…¶ä»–å¤‡é€‰æ¨¡å‹ ==========
    "deepseek-r1": {
        "name": "DeepSeek-R1",
        "hf_model": "deepseek-ai/DeepSeek-R1",
        "description": "DeepSeek æ¨ç†æ¨¡å‹",
        "min_gpus": 4,
        "recommended_gpus": 8,
        "gpu_memory": "4x80GB+",
        "default_max_len": 16384,
        "extra_args": ["--trust-remote-code"],
        "env_model_name": "deepseek-ai/DeepSeek-R1",
    },
    "llama3.1-70b": {
        "name": "Llama-3.1-70B",
        "hf_model": "meta-llama/Llama-3.1-70B-Instruct",
        "description": "Meta Llama 3.1 70B æŒ‡ä»¤æ¨¡å‹",
        "min_gpus": 2,
        "recommended_gpus": 2,
        "gpu_memory": "2x40GB+",
        "default_max_len": 8192,
        "extra_args": [],
        "env_model_name": "meta-llama/Llama-3.1-70B-Instruct",
    },
    "llama3.1-8b": {
        "name": "Llama-3.1-8B",
        "hf_model": "meta-llama/Llama-3.1-8B-Instruct",
        "description": "è½»é‡çº§ Llama æ¨¡å‹",
        "min_gpus": 1,
        "recommended_gpus": 1,
        "gpu_memory": "16GB+",
        "default_max_len": 8192,
        "extra_args": [],
        "env_model_name": "meta-llama/Llama-3.1-8B-Instruct",
    },
    "mixtral-8x22b": {
        "name": "Mixtral-8x22B",
        "hf_model": "mistralai/Mixtral-8x22B-Instruct-v0.1",
        "description": "Mistral MoE æ¨¡å‹",
        "min_gpus": 2,
        "recommended_gpus": 4,
        "gpu_memory": "2x80GB+",
        "default_max_len": 8192,
        "extra_args": [],
        "env_model_name": "mistralai/Mixtral-8x22B-Instruct-v0.1",
    },
}


def list_models():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹é…ç½®"""
    print("\n" + "=" * 70)
    print("å¯ç”¨æ¨¡å‹é…ç½®")
    print("=" * 70)
    
    # é¡¶é…æ¨¡å‹ (8x H100)
    print("\nğŸš€ é¡¶é…æ¨¡å‹ (8x H100 80GB):")
    print("-" * 70)
    for key in ["deepseek-v3-fp8", "qwen3-235b"]:
        cfg = MODEL_CONFIGS[key]
        print(f"\n  {key}")
        print(f"    æ¨¡å‹: {cfg['name']}")
        print(f"    æè¿°: {cfg['description']}")
        print(f"    GPU: {cfg['min_gpus']}x ({cfg['gpu_memory']})")
        print(f"    HuggingFace: {cfg['hf_model']}")
    
    # ä¸»è¦ç›®æ ‡æ¨¡å‹
    print("\n\nğŸ“Œ ä¸»è¦ç›®æ ‡æ¨¡å‹:")
    print("-" * 70)
    for key in ["qwen3-32b", "deepseek-v3", "qwen3-14b"]:
        cfg = MODEL_CONFIGS[key]
        print(f"\n  {key}")
        print(f"    æ¨¡å‹: {cfg['name']}")
        print(f"    æè¿°: {cfg['description']}")
        print(f"    æœ€ä½ GPU: {cfg['min_gpus']} ({cfg['gpu_memory']})")
        print(f"    HuggingFace: {cfg['hf_model']}")
    
    # å…¶ä»–æ¨¡å‹
    print("\n\nğŸ“¦ å…¶ä»–å¤‡é€‰æ¨¡å‹:")
    print("-" * 70)
    for key, cfg in MODEL_CONFIGS.items():
        if key not in ["qwen3-32b", "deepseek-v3", "qwen3-14b", "deepseek-v3-fp8", "qwen3-235b"]:
            print(f"\n  {key}")
            print(f"    æ¨¡å‹: {cfg['name']} - {cfg['description']}")
            print(f"    GPU: {cfg['min_gpus']}+ ({cfg['gpu_memory']})")


def build_vllm_command(
    model_key: str,
    port: int = 8000,
    gpus: Optional[int] = None,
    max_len: Optional[int] = None,
    gpu_util: float = 0.9,
) -> list:
    """æ„å»º vLLM å¯åŠ¨å‘½ä»¤"""
    
    if model_key not in MODEL_CONFIGS:
        print(f"Error: æœªçŸ¥æ¨¡å‹ '{model_key}'")
        print(f"å¯ç”¨æ¨¡å‹: {', '.join(MODEL_CONFIGS.keys())}")
        sys.exit(1)
    
    cfg = MODEL_CONFIGS[model_key]
    
    # ç¡®å®š GPU æ•°é‡
    num_gpus = gpus if gpus else cfg["recommended_gpus"]
    if num_gpus < cfg["min_gpus"]:
        print(f"Warning: {cfg['name']} æœ€å°‘éœ€è¦ {cfg['min_gpus']} ä¸ª GPU")
        num_gpus = cfg["min_gpus"]
    
    # ç¡®å®š context é•¿åº¦
    context_len = max_len if max_len else cfg["default_max_len"]
    
    # æ„å»ºå‘½ä»¤
    cmd = [
        "vllm", "serve", cfg["hf_model"],
        "--port", str(port),
        "--tensor-parallel-size", str(num_gpus),
        "--max-model-len", str(context_len),
        "--gpu-memory-utilization", str(gpu_util),
    ]
    
    # æ·»åŠ é¢å¤–å‚æ•°
    cmd.extend(cfg["extra_args"])
    
    return cmd


def update_env_file(model_key: str, port: int = 8000):
    """æ›´æ–° .env æ–‡ä»¶ä¸­çš„æ¨¡å‹é…ç½®"""
    
    cfg = MODEL_CONFIGS[model_key]
    env_path = Path(__file__).parent.parent / ".env"
    
    if not env_path.exists():
        # ä» .env.example å¤åˆ¶
        example_path = env_path.parent / ".env.example"
        if example_path.exists():
            env_path.write_text(example_path.read_text())
            print(f"âœ… å·²ä» .env.example åˆ›å»º .env")
        else:
            print("Error: .env.example ä¸å­˜åœ¨")
            return False
    
    # è¯»å–å½“å‰å†…å®¹
    content = env_path.read_text()
    lines = content.split("\n")
    new_lines = []
    
    # æ›´æ–°é…ç½®
    updated_keys = set()
    for line in lines:
        stripped = line.strip()
        
        # è·³è¿‡æ³¨é‡Š
        if stripped.startswith("#"):
            new_lines.append(line)
            continue
        
        # æ›´æ–° LLM_MODEL
        if stripped.startswith("LLM_MODEL=") or stripped.startswith("# LLM_MODEL="):
            if "LLM_MODEL" not in updated_keys:
                new_lines.append(f"LLM_MODEL={cfg['env_model_name']}")
                updated_keys.add("LLM_MODEL")
            continue
        
        # æ›´æ–° OPENAI_API_BASE
        if stripped.startswith("OPENAI_API_BASE=") or stripped.startswith("# OPENAI_API_BASE="):
            if "OPENAI_API_BASE" not in updated_keys:
                new_lines.append(f"OPENAI_API_BASE=http://localhost:{port}/v1")
                updated_keys.add("OPENAI_API_BASE")
            continue
        
        # æ›´æ–° OPENAI_BASE_URL
        if stripped.startswith("OPENAI_BASE_URL=") or stripped.startswith("# OPENAI_BASE_URL="):
            if "OPENAI_BASE_URL" not in updated_keys:
                new_lines.append(f"OPENAI_BASE_URL=http://localhost:{port}/v1")
                updated_keys.add("OPENAI_BASE_URL")
            continue
        
        # æ›´æ–° OPENAI_API_KEY (for vLLM)
        if stripped.startswith("OPENAI_API_KEY=") and "dummy" not in stripped:
            if "OPENAI_API_KEY" not in updated_keys:
                new_lines.append("OPENAI_API_KEY=dummy")
                updated_keys.add("OPENAI_API_KEY")
            continue
        
        new_lines.append(line)
    
    # å†™å›æ–‡ä»¶
    env_path.write_text("\n".join(new_lines))
    
    print(f"âœ… å·²æ›´æ–° .env æ–‡ä»¶:")
    print(f"   LLM_MODEL={cfg['env_model_name']}")
    print(f"   OPENAI_API_BASE=http://localhost:{port}/v1")
    
    return True


def main():
    parser = argparse.ArgumentParser(
        description="vLLM æ¨¡å‹éƒ¨ç½²è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    parser.add_argument(
        "--model", "-m",
        choices=list(MODEL_CONFIGS.keys()),
        help="è¦éƒ¨ç½²çš„æ¨¡å‹",
    )
    parser.add_argument(
        "--list", "-l",
        action="store_true",
        help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹",
    )
    parser.add_argument(
        "--port", "-p",
        type=int,
        default=8000,
        help="vLLM æœåŠ¡ç«¯å£ (default: 8000)",
    )
    parser.add_argument(
        "--gpus", "-g",
        type=int,
        help="GPU æ•°é‡ (tensor parallel size)",
    )
    parser.add_argument(
        "--max-len",
        type=int,
        help="æœ€å¤§ context é•¿åº¦",
    )
    parser.add_argument(
        "--gpu-util",
        type=float,
        default=0.9,
        help="GPU å†…å­˜åˆ©ç”¨ç‡ (default: 0.9)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="åªæ‰“å°å‘½ä»¤ï¼Œä¸æ‰§è¡Œ",
    )
    parser.add_argument(
        "--update-env",
        action="store_true",
        help="åŒæ—¶æ›´æ–° .env æ–‡ä»¶",
    )
    
    args = parser.parse_args()
    
    # åˆ—å‡ºæ¨¡å‹
    if args.list:
        list_models()
        return
    
    # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†æ¨¡å‹
    if not args.model:
        print("Error: è¯·æŒ‡å®šè¦éƒ¨ç½²çš„æ¨¡å‹ (--model)")
        print("ä½¿ç”¨ --list æŸ¥çœ‹å¯ç”¨æ¨¡å‹")
        sys.exit(1)
    
    # æ„å»ºå‘½ä»¤
    cmd = build_vllm_command(
        model_key=args.model,
        port=args.port,
        gpus=args.gpus,
        max_len=args.max_len,
        gpu_util=args.gpu_util,
    )
    
    cfg = MODEL_CONFIGS[args.model]
    
    print("\n" + "=" * 70)
    print(f"éƒ¨ç½²æ¨¡å‹: {cfg['name']}")
    print("=" * 70)
    print(f"\næè¿°: {cfg['description']}")
    print(f"HuggingFace: {cfg['hf_model']}")
    print(f"ç«¯å£: {args.port}")
    print(f"\nå‘½ä»¤:")
    print(f"  {' '.join(cmd)}")
    
    # æ›´æ–° .env
    if args.update_env:
        print("\n")
        update_env_file(args.model, args.port)
    
    # æ‰§è¡Œ
    if args.dry_run:
        print("\n[Dry run - ä¸æ‰§è¡Œå‘½ä»¤]")
    else:
        print("\nå¯åŠ¨ vLLM æœåŠ¡...")
        print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡\n")
        try:
            subprocess.run(cmd)
        except KeyboardInterrupt:
            print("\n\næœåŠ¡å·²åœæ­¢")
        except FileNotFoundError:
            print("\nError: vLLM æœªå®‰è£…")
            print("è¯·å…ˆå®‰è£…: pip install vllm")
            sys.exit(1)


if __name__ == "__main__":
    main()
