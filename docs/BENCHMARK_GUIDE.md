# AgentBusters Benchmark è¿è¡ŒæŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜å¦‚ä½•ä½¿ç”¨å®éªŒå®¤ GPU è¿è¡Œå¼€æº LLM è¿›è¡Œ benchmark æµ‹è¯•ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨ AgentBusters-Leaderboard æ”¶é›†ä¸åŒé…ç½®ä¸‹çš„æµ‹è¯•ç»“æœã€‚

## ç›®å½•

1. [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
2. [LLM é…ç½®é€‰é¡¹](#llm-é…ç½®é€‰é¡¹)
3. [è¯„æµ‹æ•°æ®é…ç½®](#è¯„æµ‹æ•°æ®é…ç½®)
4. [è¿è¡Œ Benchmark](#è¿è¡Œ-benchmark)
5. [å¤šé…ç½®å®éªŒç®¡ç†](#å¤šé…ç½®å®éªŒç®¡ç†)
6. [ç»“æœæ”¶é›†ä¸åˆ†æ](#ç»“æœæ”¶é›†ä¸åˆ†æ)

---

## ç¯å¢ƒå‡†å¤‡

### 1. åŸºç¡€å®‰è£…

```bash
# å…‹éš†ä»“åº“
cd /path/to/your/workspace/AgentBusters

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Linux/macOS
source venv/bin/activate

# Windows PowerShell
# .\.venv\Scripts\Activate.ps1

# Windows CMD
# .\.venv\Scripts\activate.bat

# å®‰è£…ä¾èµ–
pip install -e ".[dev]"

pip install vllm --extra-index-url https://download.pytorch.org/whl/cu128
```

### 2. é…ç½®æ–‡ä»¶è®¾ç½®

å¤åˆ¶ `.env.example` åˆ° `.env` å¹¶æ ¹æ®éœ€è¦ä¿®æ”¹ï¼š

```bash
cp .env.example .env
```

---

## LLM é…ç½®é€‰é¡¹

### Purple Agent LLM é…ç½®ï¼ˆåœ¨ `.env` ä¸­å®šä¹‰ï¼‰

Purple Agent æ˜¯è¢«è¯„æµ‹çš„é‡‘èåˆ†æ Agentï¼Œå…¶ LLM é…ç½®åœ¨ `.env` æ–‡ä»¶ä¸­è®¾ç½®ï¼š

```dotenv
# ============================================
# æ¨èé…ç½®: æœ¬åœ° vLLM éƒ¨ç½²ï¼ˆGPU æœåŠ¡å™¨ï¼‰
# ============================================
OPENAI_API_KEY=dummy                          # vLLM ä¸éœ€è¦çœŸå® API key
OPENAI_API_BASE=http://localhost:8000/v1      # vLLM æœåŠ¡åœ°å€
OPENAI_BASE_URL=http://localhost:8000/v1      # åˆ«å

# --- Qwen3-32Bï¼ˆæ¨èï¼Œå¹³è¡¡æ€§èƒ½ä¸èµ„æºï¼‰---
LLM_MODEL=Qwen/Qwen3-32B

# --- DeepSeek-V3.2ï¼ˆæœ€å¼ºæ€§èƒ½ï¼‰---
# LLM_MODEL=deepseek-ai/DeepSeek-V3

# --- Qwen3-14Bï¼ˆè½»é‡çº§ï¼‰---
# LLM_MODEL=Qwen/Qwen3-14B

# ============================================
# å¤‡é€‰: OpenRouter APIï¼ˆæ— éœ€æœ¬åœ° GPUï¼‰
# ============================================
# OPENAI_API_KEY=sk-or-v1-xxxxxxxxxxxxx
# OPENAI_API_BASE=https://openrouter.ai/api/v1
# LLM_MODEL=qwen/qwen3-32b              # Qwen3-32B via OpenRouter
# LLM_MODEL=deepseek/deepseek-chat      # DeepSeek via OpenRouter

# ============================================
# å•†ä¸š APIï¼ˆç”¨äºåŸºå‡†å¯¹æ¯”ï¼‰
# ============================================
# OPENAI_API_KEY=sk-...
# LLM_MODEL=gpt-4o

# LLM_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-...
# LLM_MODEL=claude-sonnet-4-20250514

# ============================================
# é€šç”¨é…ç½®
# ============================================
PURPLE_LLM_TEMPERATURE=0.0  # è®¾ä¸º 0.0 ä»¥è·å¾—å¯é‡å¤çš„ç»“æœ
```

### Green Agent LLM é…ç½®ï¼ˆåœ¨ eval_config.yaml ä¸­å®šä¹‰ï¼‰

Green Agent æ˜¯è¯„æµ‹å™¨ï¼Œä½¿ç”¨ LLM-as-judge è¿›è¡Œè¯„åˆ†ï¼š

```yaml
# config/eval_config.yaml
llm_eval:
  enabled: true
  model: gpt-4o-mini       # è¯„åˆ¤æ¨¡å‹
  temperature: 0.0         # å›ºå®šä¸º 0 ä»¥ä¿è¯å¯é‡å¤æ€§
```

### å¯åŠ¨æœ¬åœ° vLLM æœåŠ¡ï¼ˆGPU æœåŠ¡å™¨ï¼‰

```bash
# é‡è¦ï¼šç‰ˆæœ¬å…¼å®¹æ€§è¯´æ˜
# vLLM 0.15.0 ä¼šè‡ªåŠ¨å®‰è£… PyTorch 2.9.xï¼Œè¿™æ˜¯æ¨èç‰ˆæœ¬
# âš ï¸ æ³¨æ„ï¼šä¸åŒ CUDA ç´¢å¼•ä¼šå®‰è£…ä¸åŒ PyTorch ç‰ˆæœ¬ (cu126â†’2.10, cu124â†’2.4)
# æ¨èæ–¹æ³•ï¼šè®© vLLM è‡ªåŠ¨å¤„ç† PyTorch ç‰ˆæœ¬ç®¡ç†

# æ–¹æ³• 1ï¼šè®© vLLM è‡ªåŠ¨å¤„ç† PyTorch (æ¨è)
pip install vllm
# vLLM ä¼šè‡ªåŠ¨å®‰è£…å…¼å®¹çš„ PyTorch 2.9.x + CUDA æ”¯æŒ

# æ–¹æ³• 2ï¼šæ‰‹åŠ¨æŒ‡å®šç‰ˆæœ¬ (å¦‚æœéœ€è¦å®Œå…¨æ§åˆ¶)
pip install torch==2.9.1 torchvision torchaudio  # ä¸ä½¿ç”¨ç´¢å¼• URL
pip install vllm

# 1. æ£€æŸ¥å½“å‰ PyTorch æ˜¯å¦æ”¯æŒ CUDA
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')"

# å¦‚æœæ˜¾ç¤º CUDA available: False æˆ–ç‰ˆæœ¬åŒ…å« '+cpu'ï¼Œéœ€è¦é‡æ–°å®‰è£…
# å¸è½½ CPU ç‰ˆæœ¬çš„ PyTorch
pip uninstall torch torchvision torchaudio -y

# é‡æ–°å®‰è£… vLLM (ä¼šè‡ªåŠ¨å®‰è£…æ­£ç¡®çš„ PyTorch ç‰ˆæœ¬)
pip install vllm --no-cache-dir

# ä¿®å¤ NumPy å…¼å®¹æ€§é—®é¢˜ (å¦‚æœå‡ºç° NumPy 2.x è­¦å‘Š)
pip install "numpy<2.0"

# éªŒè¯å®‰è£…
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print(f'GPU count: {torch.cuda.device_count()}')"
python -c "import vllm; print('vLLM installed successfully')"
```

#### æ¨èæ¨¡å‹éƒ¨ç½²

**ğŸš€ GH200 è¶…çº§è®¡ç®— (å•å¡ 480GB HBM3e) - ç»ˆæå•å¡**

**1. DeepSeek-V3.2-671B (å•å¡ GH200-480GB) - å•å¡è·‘ 671B å…¨å‚æ•°ï¼**
```bash
# 1x GH200 480GB - å•å¡è¿è¡Œ 671B MoE å…¨ç²¾åº¦
vllm serve deepseek-ai/DeepSeek-V3 \
    --port 8000 \
    --tensor-parallel-size 1 \
    --max-model-len 131072 \
    --gpu-memory-utilization 0.90 \
    --trust-remote-code \
    --enable-auto-tool-choice \
    --tool-call-parser deepseek_v3

# å¿«é€Ÿéƒ¨ç½²å‘½ä»¤
python scripts/deploy_vllm.py --model deepseek-v3-gh200
```

**2. Qwen3-235B-A22B (å•å¡ GH200-480GB) - è¶…å¤§ Context**
```bash
# 1x GH200 480GB - å•å¡è¿è¡Œ 235B MoE
vllm serve Qwen/Qwen3-235B-A22B \
    --port 8000 \
    --tensor-parallel-size 1 \
    --max-model-len 40960 \
    --gpu-memory-utilization 0.90 \
    --trust-remote-code \
    --enable-auto-tool-choice \
    --tool-call-parser qwen3_xml

# æ³¨æ„: Qwen3-235B-A22B çš„åŸç”Ÿä¸Šä¸‹æ–‡é•¿åº¦ä¸º 40,960 tokens
# GH200-480GB æœ‰è¶³å¤Ÿå†…å­˜ï¼Œä½†æ¨¡å‹æ¶æ„é™åˆ¶äº†ä¸Šä¸‹æ–‡é•¿åº¦

# âš ï¸ å•å¡ H100 80GB æ— æ³•è¿è¡Œæ­¤æ¨¡å‹ï¼éœ€è¦è‡³å°‘ 3 å¼  H100 80GB
# 8x H100 80GB é…ç½® (æ¨è)
vllm serve Qwen/Qwen3-235B-A22B \
    --port 8000 \
    --tensor-parallel-size 8 \
    --max-model-len 40960 \
    --gpu-memory-utilization 0.90 \
    --trust-remote-code \
    --enable-auto-tool-choice \
    --tool-call-parser qwen3_xml

# 3x H100 80GB é…ç½® (æœ€å°é…ç½®)
vllm serve Qwen/Qwen3-235B-A22B \
    --port 8000 \
    --tensor-parallel-size 3 \
    --max-model-len 32768 \
    --gpu-memory-utilization 0.90 \
    --trust-remote-code \
    --enable-auto-tool-choice \
    --tool-call-parser qwen3_xml

# å¿«é€Ÿéƒ¨ç½²å‘½ä»¤
python scripts/deploy_vllm.py --model qwen3-235b-gh200
```

---

**ğŸ”‹ GH200 æ ‡å‡†ç‰ˆ (å•å¡ 96GB HBM3e) - é«˜æ•ˆå•å¡**

**âš ï¸ é‡è¦æç¤º: Qwen3-235B-A22B å®é™…éœ€è¦è¶…è¿‡ 96GB å†…å­˜ï¼Œæ— æ³•åœ¨ GH200-96GB ä¸Šè¿è¡Œ** 

**æ¨èæ›¿ä»£æ–¹æ¡ˆ:**
```bash
# 1. Qwen3-32B (æœ€ä½³é€‰æ‹©) - å•å¡è¿è¡Œï¼Œæ€§èƒ½ä¼˜ç§€
vllm serve Qwen/Qwen3-32B \
    --port 8000 \
    --tensor-parallel-size 1 \
    --max-model-len 32768 \
    --gpu-memory-utilization 0.90 \
    --enable-auto-tool-choice \
    --tool-call-parser qwen3_xml

# 2. DeepSeek-V3 é‡åŒ–ç‰ˆæœ¬ (å®éªŒæ€§)
# æ³¨æ„: å³ä½¿é‡åŒ–ä¹Ÿå¯èƒ½è¶…å‡º 96GB é™åˆ¶
vllm serve deepseek-ai/DeepSeek-V3 \
    --port 8000 \
    --tensor-parallel-size 1 \
    --max-model-len 16384 \
    --gpu-memory-utilization 0.85 \
    --quantization gptq \
    --trust-remote-code \
    --enable-auto-tool-choice \
    --tool-call-parser deepseek_v3
```

---

**âš¡ B200 é¡¶é… (å•å¡ 192GB HBM3e) - æœ€å¼ºå•å¡**

**1. Qwen3-235B-A22B (å•å¡ B200) - å•å¡è·‘ 235Bï¼**
```bash
# 1x B200 192GB - å•å¡è¿è¡Œ 235B MoE
vllm serve Qwen/Qwen3-235B-A22B \
    --port 8000 \
    --tensor-parallel-size 1 \
    --max-model-len 40960 \
    --gpu-memory-utilization 0.95 \
    --trust-remote-code \
    --enable-auto-tool-choice \
    --tool-call-parser qwen3_xml

# å¿«é€Ÿéƒ¨ç½²å‘½ä»¤
python scripts/deploy_vllm.py --model qwen3-235b-b200
```

**2. DeepSeek-V3.2-671B (3x B200) - å…¨ç²¾åº¦ 671B**
```bash
# 3x B200 192GB - BF16 å…¨ç²¾åº¦è¿è¡Œ 671B MoE
vllm serve deepseek-ai/DeepSeek-V3 \
    --port 8000 \
    --tensor-parallel-size 3 \
    --max-model-len 65536 \
    --gpu-memory-utilization 0.95 \
    --trust-remote-code \
    --enable-auto-tool-choice \
    --tool-call-parser deepseek_v3

# å¿«é€Ÿéƒ¨ç½²å‘½ä»¤
python scripts/deploy_vllm.py --model deepseek-v3-b200
```

---

**ğŸš€ H100 é¡¶é… (8x 80GB) - âš ï¸ DeepSeek-V3 éœ€è¦ Hopper GPU**

> **âš ï¸ é‡è¦ç¡¬ä»¶è¦æ±‚**: DeepSeek-V3 ä½¿ç”¨ MLA (Multi-head Latent Attention) æ¶æ„ï¼Œ**å¿…é¡»ä½¿ç”¨ Hopper æ¶æ„ GPU (H100/H200)**ã€‚
> A100 (compute capability 8.0) **æ— æ³•è¿è¡Œ** DeepSeek-V3ï¼Œå³ä½¿æœ‰ 8 å¼ å¡ä¹Ÿä¸è¡Œï¼
> 
> å¦‚æœæ‚¨ä½¿ç”¨ A100ï¼Œè¯·ä½¿ç”¨ Qwen3-32Bã€Llama-3.1-70B æˆ– Mixtral-8x22B ç­‰æ›¿ä»£æ¨¡å‹ã€‚

**1. DeepSeek-V3.2-671B (FP8 é‡åŒ–) - ä»…é™ H100/H200**
```bash
# 8x H100 80GB, FP8 é‡åŒ– - æ¨èé…ç½® (16K contextï¼Œç¨³å®šè¿è¡Œ)
# âš ï¸ æ­¤å‘½ä»¤ä»…é€‚ç”¨äº H100/H200 GPUï¼A100 æ— æ³•è¿è¡Œï¼
vllm serve deepseek-ai/DeepSeek-V3.2 \
  --port 8000 \
  --tensor-parallel-size 8 \
  --max-model-len 24576 \
  --gpu-memory-utilization 0.8 \
  --quantization fp8 \
  --kv-cache-dtype fp8_e4m3 \
  --dtype bfloat16 \
  --enable-auto-tool-choice \
  --tool-call-parser deepseek_v3

# 8x H100 80GB, FP8 é‡åŒ– - è¾ƒå¤§ context (24Kï¼Œé™ä½å†…å­˜åˆ©ç”¨ç‡)
# å¦‚æœ 16K ä¸å¤Ÿï¼Œå¯ä»¥å°è¯•æ­¤é…ç½®
vllm serve deepseek-ai/DeepSeek-V3 \
    --port 8000 \
    --tensor-parallel-size 8 \
    --max-model-len 24576 \
    --gpu-memory-utilization 0.95 \
    --trust-remote-code \
    --quantization fp8 \
    --kv-cache-dtype fp8_e4m3 \
    --enable-auto-tool-choice \
    --tool-call-parser deepseek_v3

# âš ï¸ æ³¨æ„: 32K context + 0.95 å†…å­˜åˆ©ç”¨ç‡ä¼š OOMï¼
# å¦‚æœéœ€è¦ 32K contextï¼Œè¯·ä½¿ç”¨ 0.85 å†…å­˜åˆ©ç”¨ç‡æˆ–æ›´å¤š GPU

# å¦‚æœä¸ä½¿ç”¨ FP8 é‡åŒ– (BF16ï¼Œéœ€è¦æ›´å¤šæ˜¾å­˜)
vllm serve deepseek-ai/DeepSeek-V3 \
    --port 8000 \
    --tensor-parallel-size 8 \
    --max-model-len 8192 \
    --gpu-memory-utilization 0.95 \
    --trust-remote-code \
    --dtype bfloat16 \
    --enable-auto-tool-choice \
    --tool-call-parser deepseek_v3
```

---

**ï¿½ TensorRT-LLM + NVFP4 (æ¨è: æœ€é«˜åå)**

> **æ¨èè·¯çº¿**: å¯¹äº DeepSeek-V3.2ï¼ŒTRT-LLM + NVFP4 æ˜¯ NVIDIA å®˜æ–¹æ¨èçš„éƒ¨ç½²æ–¹å¼ï¼Œæ¯” vLLM æœ‰æ›´é«˜ååé‡ã€‚
> NVFP4 æ˜¯é¢„é‡åŒ–æ¨¡å‹ï¼Œä¸éœ€è¦è‡ªå·± build engineï¼Œå¼€ç®±å³ç”¨ã€‚

**1. DeepSeek-V3.2-NVFP4 (8x H100 80GB) - æœ€ä¼˜æ–¹æ¡ˆ**
```bash
# ä½¿ç”¨éƒ¨ç½²è„šæœ¬ (æ¨è)
python scripts/deploy_trtllm.py --model deepseek-v3-nvfp4 --port 8000

# æˆ–æ‰‹åŠ¨éƒ¨ç½²:
# Step 1: å¯åŠ¨ TRT-LLM å®¹å™¨
docker run --rm -it \
  --gpus all \
  --ipc=host \
  --ulimit memlock=-1 \
  --ulimit stack=67108864 \
  -p 8000:8000 \
  -v /mnt/models:/models \
  -v $(pwd)/src/trtllm_api:/app \
  -w /app \
  nvcr.io/nvidia/tensorrt-llm/release:1.3.0rc1

# Step 2: åœ¨å®¹å™¨å†…å¯åŠ¨ API
pip install fastapi uvicorn pydantic
python trtllm_openai_api.py \
    --model nvidia/DeepSeek-V3.2-NVFP4 \
    --tensor-parallel-size 8 \
    --port 8000
```

**TRT-LLM vs vLLM å¯¹æ¯”:**

| å¯¹æ¯”é¡¹ | vLLM | TRT-LLM + NVFP4 |
|--------|------|-----------------|
| DeepSeek-V3.2 æ”¯æŒ | âœ… | âœ… |
| FP8/FP4 MoE | âŒ | âœ… |
| 8Ã—80GB ç¨³å®šæ€§ | ä¸€èˆ¬ | **ç¨³å®š** |
| ååé‡ | é«˜ | **æ›´é«˜** |
| å·¥ç¨‹å¤æ‚åº¦ | ä½ | ä¸­ |
| ç”Ÿäº§å¯æ§æ€§ | ä¸­ | **é«˜** |

---

**ï¿½ğŸ’ A100 é¡¶é… (8x 80GB) - æ¨èé…ç½®**

> A100 ç”¨æˆ·æ¨èä½¿ç”¨ä»¥ä¸‹æ¨¡å‹ (ä¸æ”¯æŒ DeepSeek-V3)

**1. Qwen3-32B (æ¨è) - æ€§èƒ½ä¼˜ç§€ï¼Œèµ„æºå‹å¥½**
```bash
# 8x A100 80GB - å¯ä»¥è¿è¡Œå¤šå®ä¾‹æˆ–ä½¿ç”¨æ›´å¤§ context
vllm serve Qwen/Qwen3-32B \
    --port 8000 \
    --tensor-parallel-size 2 \
    --max-model-len 32768 \
    --gpu-memory-utilization 0.90 \
    --enable-auto-tool-choice \
    --tool-call-parser qwen3_xml
```

**2. Llama-3.1-70B-Instruct - å¼ºå¤§çš„é€šç”¨æ¨¡å‹**
```bash
# 8x A100 80GB
vllm serve meta-llama/Llama-3.1-70B-Instruct \
    --port 8000 \
    --tensor-parallel-size 2 \
    --max-model-len 32768 \
    --gpu-memory-utilization 0.90 \
    --enable-auto-tool-choice \
    --tool-call-parser llama3_json
```

**3. Mixtral-8x22B-Instruct - MoE æ¶æ„ (æ—  MLA é™åˆ¶)**
```bash
# 8x A100 80GB
vllm serve mistralai/Mixtral-8x22B-Instruct-v0.1 \
    --port 8000 \
    --tensor-parallel-size 4 \
    --max-model-len 32768 \
    --gpu-memory-utilization 0.90 \
    --enable-auto-tool-choice \
    --tool-call-parser mistral
```

**2. Qwen3-235B-A22B (MoE) - é¡¶çº§ MoE**
```bash
# 8x H100 80GB, 235B å‚æ•° (22B æ¿€æ´»)
vllm serve Qwen/Qwen3-235B-A22B \
    --port 8000 \
    --tensor-parallel-size 8 \
    --max-model-len 40960 \
    --gpu-memory-utilization 0.95 \
    --trust-remote-code \
    --enable-auto-tool-choice \
    --tool-call-parser qwen3_xml
```

---

**ğŸ“Œ ä¸»è¦ç›®æ ‡æ¨¡å‹**

**1. Qwen3-32Bï¼ˆæ¨èï¼Œå¹³è¡¡æ€§èƒ½ä¸èµ„æºï¼‰**
```bash
# å• GPU (A100 80GB / H100 80GB)
vllm serve Qwen/Qwen3-32B \
    --port 8000 \
    --tensor-parallel-size 1 \
    --max-model-len 32768 \
    --gpu-memory-utilization 0.9 \
    --enable-auto-tool-choice \
    --tool-call-parser qwen3_xml

2 x A6000
CUDA_VISIBLE_DEVICES=2,3 vllm serve Qwen/Qwen3-32B --port 8100 --tensor-parallel-size 2 --max-model-len 16384 --enable-auto-tool-choice --tool-call-parser qwen3_xml

# åŒ GPU (2x A100 40GB / 2x RTX 4090)
vllm serve Qwen/Qwen3-32B \
    --port 8000 \
    --tensor-parallel-size 2 \
    --max-model-len 16384 \
    --enable-auto-tool-choice \
    --tool-call-parser qwen3_xml
```

**2. DeepSeek-V3.2ï¼ˆé«˜æ€§èƒ½ï¼Œéœ€è¦å¤š GPUï¼‰**
```bash
# 4x A100 80GB æˆ– 8x A100 40GB
vllm serve deepseek-ai/DeepSeek-V3 \
    --port 8000 \
    --tensor-parallel-size 4 \
    --max-model-len 16384 \
    --gpu-memory-utilization 0.95 \
    --trust-remote-code \
    --enable-auto-tool-choice \
    --tool-call-parser deepseek_v3

# 8x GPU é…ç½®ï¼ˆæ›´å¤§ contextï¼‰
vllm serve deepseek-ai/DeepSeek-V3 \
    --port 8000 \
    --tensor-parallel-size 8 \
    --max-model-len 32768 \
    --trust-remote-code \
    --enable-auto-tool-choice \
    --tool-call-parser deepseek_v3
```

**3. Qwen3-14Bï¼ˆè½»é‡çº§ï¼Œé€‚åˆå• GPUï¼‰**
```bash
# å• GPU (RTX 4090 24GB / A100 40GB)
vllm serve Qwen/Qwen3-14B \
    --port 8000 \
    --tensor-parallel-size 1 \
    --max-model-len 32768 \
    --gpu-memory-utilization 0.9 \
    --enable-auto-tool-choice \
    --tool-call-parser qwen3_xml

# RTX 3090 24GBï¼ˆå‡å°‘ context é•¿åº¦ï¼‰
vllm serve Qwen/Qwen3-14B \
    --port 8000 \
    --max-model-len 8192 \
    --gpu-memory-utilization 0.95 \
    --enable-auto-tool-choice \
    --tool-call-parser qwen3_xml
```

#### GPU å†…å­˜éœ€æ±‚å‚è€ƒ

| æ¨¡å‹ | å‚æ•°é‡ | æœ€ä½ GPU | æ¨èé…ç½® | Context é•¿åº¦ |
|------|--------|----------|----------|-------------|
| Qwen3-14B | 14B | 1x RTX 4090 (24GB) | 1x A100 40GB | 32K |
| Qwen3-32B | 32B | 2x RTX 4090 | 1x A100 80GB | 32K |
| DeepSeek-V3.2 | 671B MoE | 4x A100 80GB | 8x H100 80GB | 32K |
| Qwen3-235B-A22B | 235B MoE (22Bæ¿€æ´») | 1x GH200 480GB or 2x H100 80GB | 1x GH200-480GB | 40K |
| DeepSeek-V3 FP8 | 671B MoE | 8x H100 80GB | 8x H100 80GB | 32K |
| **âš¡ Qwen3-235B (B200)** | **235B MoE** | **1x B200 192GB** | **1x B200** | **40K** |
| **âš¡ DeepSeek-V3 (B200)** | **671B MoE** | **3x B200 192GB** | **3x B200** | **65K** |
| **ğŸš€ Qwen3-32B (GH200-96GB)** | **32B** | **1x GH200 96GB** | **1x GH200 96GB** | **32K** |
| **ğŸš€ Qwen3-235B (GH200-480GB)** | **235B MoE** | **1x GH200 480GB** | **1x GH200** | **40K** |
| **ğŸš€ DeepSeek-V3 (GH200-480GB)** | **671B MoE** | **1x GH200 480GB** | **1x GH200** | **131K** |

#### å…¶ä»–æ¨¡å‹ï¼ˆå¤‡é€‰ï¼‰
```bash
# Llama 3.1 70B
vllm serve meta-llama/Llama-3.1-70B-Instruct \
    --port 8000 \
    --tensor-parallel-size 2 \
    --enable-auto-tool-choice \
    --tool-call-parser llama3_json

# Mixtral 8x22B
vllm serve mistralai/Mixtral-8x22B-Instruct-v0.1 \
    --port 8000 \
    --tensor-parallel-size 2 \
    --enable-auto-tool-choice \
    --tool-call-parser mistral
```

---

## è¯„æµ‹æ•°æ®é…ç½®

### è¯„æµ‹é…ç½®æ–‡ä»¶ç»“æ„

è¯„æµ‹é…ç½®åœ¨ `config/` ç›®å½•ä¸‹çš„ YAML æ–‡ä»¶ä¸­å®šä¹‰ã€‚ä»¥ä¸‹æ˜¯ä¸åŒè§„æ¨¡çš„é…ç½®ç¤ºä¾‹ï¼š

### å¿«é€Ÿæµ‹è¯•é…ç½®ï¼ˆ~10 tasksï¼‰

åˆ›å»º `config/eval_quick_test.yaml`:

```yaml
name: "Quick Test (10 tasks)"
version: "1.0"

datasets:
  - type: bizfinbench
    task_types: [event_logic_reasoning]
    languages: [en]
    limit_per_task: 3
    shuffle: false
    weight: 1.0

  - type: synthetic
    path: data/synthetic_questions/questions.json
    limit: 4
    shuffle: false
    weight: 1.0

  - type: options
    path: data/options/questions.json
    limit: 3
    shuffle: false
    weight: 1.0

sampling:
  strategy: stratified
  total_limit: 10
  seed: 42

llm_eval:
  enabled: true
  model: gpt-4o-mini
  temperature: 0.0

timeout_seconds: 300
```

### ä¸­ç­‰è§„æ¨¡é…ç½®ï¼ˆ~100 tasksï¼‰

åˆ›å»º `config/eval_medium.yaml`:

```yaml
name: "Medium Scale Evaluation (100 tasks)"
version: "1.0"

datasets:
  - type: bizfinbench
    task_types:
      - event_logic_reasoning
      - financial_quantitative_computation
      - anomaly_information_tracing
    languages: [en, cn]
    limit_per_task: 8
    shuffle: false
    weight: 1.0

  - type: prbench
    splits: [finance, legal]
    limit: 20
    shuffle: false
    weight: 1.0

  - type: synthetic
    path: data/synthetic_questions/questions.json
    limit: 20
    shuffle: false
    weight: 1.0

  - type: options
    path: data/options/questions.json
    limit: 20
    shuffle: false
    weight: 1.0

  - type: crypto
    path: ../agentbusters-eval-data/crypto/eval_hidden  # ä½¿ç”¨ eval-data ä¸­çš„æ•°æ®
    download_on_missing: false
    limit: 6
    shuffle: false
    weight: 1.0
    stride: 1
    max_steps: 100
    evaluation:
      initial_balance: 10000.0
      max_leverage: 3.0
      trading_fee: 0.0004
      price_noise_level: 0.001
      slippage_range: [0.0002, 0.0010]
      adversarial_injection_rate: 0.05
      decision_interval: 5
      funding_interval_hours: 8.0
      score_weights:
        baseline: 0.40
        noisy: 0.30
        adversarial: 0.20
        meta: 0.10
      metric_weights:
        sharpe: 0.50
        total_return: 0.25
        max_drawdown: 0.15
        win_rate: 0.10

  - type: gdpval
    hf_dataset: "openai/gdpval"
    limit: 10
    shuffle: false
    weight: 1.0

sampling:
  strategy: stratified
  total_limit: 100
  seed: 42

llm_eval:
  enabled: true
  model: gpt-4o-mini
  temperature: 0.0

timeout_seconds: 600
```

### å¤§è§„æ¨¡é…ç½®ï¼ˆ~1000 tasksï¼‰

åˆ›å»º `config/eval_large.yaml`:

```yaml
name: "Large Scale Evaluation (1000 tasks)"
version: "1.0"

datasets:
  - type: bizfinbench
    task_types:
      - anomaly_information_tracing
      - event_logic_reasoning
      - financial_data_description
      - financial_quantitative_computation
      - user_sentiment_analysis
      - stock_price_predict
      - financial_multi_turn_perception
    languages: [en, cn]
    limit_per_task: 50   # 7 types Ã— 2 languages Ã— 50 = 700
    shuffle: true
    weight: 1.0

  - type: prbench
    splits: [finance, legal, finance_hard, legal_hard]
    limit: 100
    shuffle: true
    weight: 1.0

  - type: synthetic
    path: data/synthetic_questions/questions.json
    limit: 50
    shuffle: true
    weight: 1.0

  - type: options
    path: data/options/questions.json
    limit: 50
    shuffle: true
    weight: 1.0

  - type: crypto
    path: ../agentbusters-eval-data/crypto/eval_hidden
    download_on_missing: false
    limit: 12  # å…¨éƒ¨ 12 ä¸ª scenarios
    shuffle: false
    weight: 1.0
    stride: 1
    max_steps: 200
    evaluation:
      initial_balance: 10000.0
      max_leverage: 3.0
      trading_fee: 0.0004
      price_noise_level: 0.001
      slippage_range: [0.0002, 0.0010]
      adversarial_injection_rate: 0.05
      decision_interval: 1
      funding_interval_hours: 8.0
      score_weights:
        baseline: 0.40
        noisy: 0.30
        adversarial: 0.20
        meta: 0.10
      metric_weights:
        sharpe: 0.50
        total_return: 0.25
        max_drawdown: 0.15
        win_rate: 0.10
      meta_transforms:
        - identity
        - scale_1_1
        - invert_returns

  - type: gdpval
    hf_dataset: "openai/gdpval"
    limit: 50
    shuffle: true
    weight: 1.0
    include_reference_files: true

sampling:
  strategy: stratified
  total_limit: 1000
  seed: 42

llm_eval:
  enabled: true
  model: gpt-4o-mini
  temperature: 0.0

timeout_seconds: 900
```

### ä½¿ç”¨ agentbusters-eval-data ä¸­çš„ Crypto æ•°æ®

ç¡®ä¿ crypto æ•°æ®è·¯å¾„æ­£ç¡®æŒ‡å‘ `agentbusters-eval-data`:

```yaml
- type: crypto
  path: ../agentbusters-eval-data/crypto/eval_hidden  # ç›¸å¯¹è·¯å¾„
  # æˆ–ä½¿ç”¨ç»å¯¹è·¯å¾„
  # path: d:/code/finbenchmark/agentbusters-eval-data/crypto/eval_hidden
```

å¯ç”¨çš„ crypto scenarios (å…± 12 ä¸ª):
- `scenario_520d87ed7569f147` (BTCUSDT)
- `scenario_b8aba67d7bfcc3b4` (BTCUSDT)
- `scenario_0a9c24d037aaa15c` (BTCUSDT)
- `scenario_9a1f49ebc9fcc664` (ETHUSDT)
- `scenario_a9d7b02930d276f2` (ETHUSDT)
- ... ç­‰

---

## è¿è¡Œ Benchmark

### æ–¹æ³• 1: æœ¬åœ°è¿è¡Œï¼ˆæ¨èç”¨äºå¼€å‘å’Œè°ƒè¯•ï¼‰

```bash
# ç»ˆç«¯ 1: å¯åŠ¨ Green Agent (è¯„æµ‹å™¨)
python src/cio_agent/a2a_server.py \
    --host 0.0.0.0 \
    --port 9109 \
    --eval-config config/eval_medium.yaml \
    --store-predicted \
    --predicted-max-chars 200

python src/cio_agent/a2a_server.py \
    --host 0.0.0.0 \
    --port 9109 \
    --eval-config config/eval_quick_test.yaml \
    --store-predicted \
    --predicted-max-chars 200

# ç»ˆç«¯ 2: å¯åŠ¨ Purple Agent (è¢«è¯„æµ‹çš„ Agent)
purple-agent serve --host 0.0.0.0 --port 9110 --card-url http://127.0.0.1:9110

# ç»ˆç«¯ 3: è¿è¡Œè¯„æµ‹
python scripts/run_a2a_eval.py \
    --green-url http://127.0.0.1:9109 \
    --purple-url http://127.0.0.1:9110 \
    --num-tasks 100 \
    --timeout 1800 \
    -v \
    -o results/eval_medium_$(date +%Y%m%d_%H%M%S).json
```

### æ–¹æ³• 2: Docker è¿è¡Œ

```bash
# æ„å»ºé•œåƒ
docker build -t agentbusters-green -f Dockerfile.green .
docker build -t agentbusters-purple -f Dockerfile.purple .

# è¿è¡Œ
docker-compose up
```

### æ–¹æ³• 3: ä½¿ç”¨ Leaderboard æ¡†æ¶

å‚è§ä¸‹ä¸€èŠ‚ [å¤šé…ç½®å®éªŒç®¡ç†](#å¤šé…ç½®å®éªŒç®¡ç†)ã€‚

---

## å¤šé…ç½®å®éªŒç®¡ç†

ä½¿ç”¨ AgentBusters-Leaderboard æ¡†æ¶æ¥ç³»ç»Ÿåœ°ç®¡ç†ä¸åŒé…ç½®ä¸‹çš„å®éªŒç»“æœã€‚

### å®éªŒé…ç½®æ¨¡æ¿

åˆ›å»º `experiments/experiment_configs.yaml`:

```yaml
# å®éªŒé…ç½®å®šä¹‰
experiments:
  # å®éªŒ 1: ä¸åŒæ¨¡å‹å¯¹æ¯”
  - name: "model_comparison"
    description: "Compare different LLM models"
    configs:
      - id: "llama3.1-70b"
        llm_model: "meta-llama/llama-3.1-70b-instruct"
        eval_config: "config/eval_medium.yaml"
        num_tasks: 100
        
      - id: "qwen2.5-72b"
        llm_model: "qwen/qwen-2.5-72b-instruct"
        eval_config: "config/eval_medium.yaml"
        num_tasks: 100
        
      - id: "deepseek-chat"
        llm_model: "deepseek/deepseek-chat"
        eval_config: "config/eval_medium.yaml"
        num_tasks: 100
        
      - id: "mixtral-8x22b"
        llm_model: "mistralai/mixtral-8x22b-instruct"
        eval_config: "config/eval_medium.yaml"
        num_tasks: 100

  # å®éªŒ 2: ä¸åŒä»»åŠ¡æ•°é‡å¯¹æ¯”
  - name: "scale_comparison"
    description: "Compare evaluation at different scales"
    configs:
      - id: "scale-10"
        llm_model: "meta-llama/llama-3.1-70b-instruct"
        eval_config: "config/eval_quick_test.yaml"
        num_tasks: 10
        
      - id: "scale-100"
        llm_model: "meta-llama/llama-3.1-70b-instruct"
        eval_config: "config/eval_medium.yaml"
        num_tasks: 100
        
      - id: "scale-500"
        llm_model: "meta-llama/llama-3.1-70b-instruct"
        eval_config: "config/eval_large.yaml"
        num_tasks: 500

  # å®éªŒ 3: æŠ½æ ·ç­–ç•¥å¯¹æ¯”
  - name: "sampling_comparison"
    description: "Compare different sampling strategies"
    configs:
      - id: "stratified"
        sampling_strategy: "stratified"
        num_tasks: 100
        
      - id: "random"
        sampling_strategy: "random"
        num_tasks: 100
        
      - id: "sequential"
        sampling_strategy: "sequential"
        num_tasks: 100
```

### æ‰¹é‡è¿è¡Œè„šæœ¬

åˆ›å»º `scripts/run_experiments.py`:

```python
#!/usr/bin/env python3
"""
æ‰¹é‡è¿è¡Œå¤šé…ç½®å®éªŒ

Usage:
    python scripts/run_experiments.py --experiment model_comparison
    python scripts/run_experiments.py --all
"""

import argparse
import json
import os
import subprocess
import sys
from datetime import datetime
from pathlib import Path

import yaml


def load_experiment_configs(config_path: str) -> dict:
    """åŠ è½½å®éªŒé…ç½®"""
    with open(config_path) as f:
        return yaml.safe_load(f)


def run_single_experiment(
    config_id: str,
    llm_model: str,
    eval_config: str,
    num_tasks: int,
    output_dir: str,
    green_url: str = "http://localhost:9109",
    purple_url: str = "http://localhost:9110",
    timeout: int = 1800,
) -> dict:
    """è¿è¡Œå•ä¸ªå®éªŒé…ç½®"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = Path(output_dir) / f"{config_id}_{timestamp}.json"
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    env = os.environ.copy()
    env["LLM_MODEL"] = llm_model
    
    # æ„å»ºå‘½ä»¤
    cmd = [
        sys.executable,
        "scripts/run_a2a_eval.py",
        "--green-url", green_url,
        "--purple-url", purple_url,
        "--num-tasks", str(num_tasks),
        "--timeout", str(timeout),
        "-v",
        "-o", str(output_file),
    ]
    
    print(f"\n{'='*60}")
    print(f"Running experiment: {config_id}")
    print(f"  Model: {llm_model}")
    print(f"  Tasks: {num_tasks}")
    print(f"  Output: {output_file}")
    print(f"{'='*60}\n")
    
    result = subprocess.run(cmd, env=env, capture_output=False)
    
    return {
        "config_id": config_id,
        "llm_model": llm_model,
        "num_tasks": num_tasks,
        "output_file": str(output_file),
        "success": result.returncode == 0,
        "timestamp": timestamp,
    }


def run_experiment_suite(
    experiment_name: str,
    configs: list,
    output_dir: str,
) -> list:
    """è¿è¡Œä¸€ç»„å®éªŒ"""
    
    results = []
    for config in configs:
        result = run_single_experiment(
            config_id=config["id"],
            llm_model=config.get("llm_model", os.getenv("LLM_MODEL", "gpt-4o")),
            eval_config=config.get("eval_config", "config/eval_config.yaml"),
            num_tasks=config.get("num_tasks", 100),
            output_dir=output_dir,
        )
        results.append(result)
    
    return results


def main():
    parser = argparse.ArgumentParser(description="Run multiple experiment configurations")
    parser.add_argument("--config", default="experiments/experiment_configs.yaml")
    parser.add_argument("--experiment", help="Specific experiment to run")
    parser.add_argument("--all", action="store_true", help="Run all experiments")
    parser.add_argument("--output-dir", default="results/experiments")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½é…ç½®
    config = load_experiment_configs(args.config)
    
    all_results = []
    
    for experiment in config["experiments"]:
        if args.all or args.experiment == experiment["name"]:
            print(f"\n{'#'*60}")
            print(f"# Experiment: {experiment['name']}")
            print(f"# {experiment['description']}")
            print(f"{'#'*60}")
            
            results = run_experiment_suite(
                experiment["name"],
                experiment["configs"],
                args.output_dir,
            )
            all_results.extend(results)
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    summary_file = Path(args.output_dir) / "experiment_summary.json"
    with open(summary_file, "w") as f:
        json.dump(all_results, f, indent=2)
    
    print(f"\nâœ… Experiment summary saved to: {summary_file}")


if __name__ == "__main__":
    main()
```

### Leaderboard ç»“æœæ”¶é›†

ä¿®æ”¹ `AgentBusters-Leaderboard/scenario.toml` æ¥å®šä¹‰ä¸åŒçš„å®éªŒ:

```toml
# scenario.toml - å¤šé…ç½®å®éªŒç¤ºä¾‹

[green_agent]
agentbeats_id = "019bc421-99d0-7ee3-ae27-658145eff474"
env = { 
    OPENAI_API_KEY = "${OPENAI_API_KEY}", 
    EVAL_CONFIG = "config/eval_medium.yaml",  # å¯ä¿®æ”¹ä¸ºä¸åŒçš„é…ç½®
    EVAL_DATA_REPO = "${EVAL_DATA_REPO}", 
    EVAL_DATA_PAT = "${EVAL_DATA_PAT}" 
}

[[participants]]
agentbeats_id = "019c16a8-a0b2-77c3-aae3-8e0c23ca5de1"
name = "purple_agent"
env = { 
    OPENAI_API_KEY = "${OPENAI_API_KEY}",
    OPENAI_API_BASE = "https://openrouter.ai/api/v1",  # æˆ–æœ¬åœ° vLLM
    LLM_MODEL = "meta-llama/llama-3.1-70b-instruct"   # è¦æµ‹è¯•çš„æ¨¡å‹
}

[config]
num_tasks = 100              # ä»»åŠ¡æ•°é‡
conduct_debate = false
timeout_seconds = 600
datasets = ["bizfinbench", "synthetic", "options", "crypto", "gdpval"]
sampling_strategy = "stratified"

# æ•°æ®é›†é™åˆ¶
bizfinbench_limit = 30
synthetic_limit = 20
options_limit = 20
crypto_limit = 12
gdpval_limit = 18
```

---

## ç»“æœæ”¶é›†ä¸åˆ†æ

### ç»“æœæ–‡ä»¶æ ¼å¼

æ¯æ¬¡è¿è¡Œä¼šç”Ÿæˆ JSON æ ¼å¼çš„ç»“æœæ–‡ä»¶ï¼š

```json
{
  "timestamp": "2026-02-02T10:30:00Z",
  "config": {
    "llm_model": "meta-llama/llama-3.1-70b-instruct",
    "num_tasks": 100,
    "eval_config": "config/eval_medium.yaml"
  },
  "results": {
    "overall_score": 65.4,
    "section_scores": {
      "knowledge": 70.2,
      "analysis": 62.5,
      "options": 58.3,
      "crypto": 71.8
    },
    "dataset_scores": {
      "bizfinbench": 0.72,
      "synthetic": 0.58,
      "options": 0.55,
      "crypto": 0.68,
      "gdpval": 0.61
    }
  }
}
```

### ç»“æœæ±‡æ€»è„šæœ¬

åˆ›å»º `scripts/aggregate_results.py`:

```python
#!/usr/bin/env python3
"""æ±‡æ€»å¤šæ¬¡å®éªŒç»“æœ"""

import json
import sys
from pathlib import Path
import pandas as pd


def load_results(results_dir: str) -> list:
    """åŠ è½½æ‰€æœ‰ç»“æœæ–‡ä»¶"""
    results = []
    for file in Path(results_dir).glob("*.json"):
        if file.name == "experiment_summary.json":
            continue
        with open(file) as f:
            data = json.load(f)
            data["filename"] = file.name
            results.append(data)
    return results


def create_summary_table(results: list) -> pd.DataFrame:
    """åˆ›å»ºæ±‡æ€»è¡¨æ ¼"""
    rows = []
    for r in results:
        config = r.get("config", {})
        scores = r.get("results", {})
        rows.append({
            "Model": config.get("llm_model", "unknown"),
            "Tasks": config.get("num_tasks", 0),
            "Overall": scores.get("overall_score", 0),
            "Knowledge": scores.get("section_scores", {}).get("knowledge", 0),
            "Analysis": scores.get("section_scores", {}).get("analysis", 0),
            "Options": scores.get("section_scores", {}).get("options", 0),
            "Crypto": scores.get("section_scores", {}).get("crypto", 0),
            "File": r.get("filename", ""),
        })
    
    df = pd.DataFrame(rows)
    df = df.sort_values("Overall", ascending=False)
    return df


def main():
    if len(sys.argv) < 2:
        print("Usage: python aggregate_results.py <results_dir>")
        sys.exit(1)
    
    results = load_results(sys.argv[1])
    df = create_summary_table(results)
    
    print("\n" + "="*80)
    print("BENCHMARK RESULTS SUMMARY")
    print("="*80)
    print(df.to_string(index=False))
    
    # ä¿å­˜ä¸º CSV
    output_file = Path(sys.argv[1]) / "leaderboard.csv"
    df.to_csv(output_file, index=False)
    print(f"\nâœ… Saved to: {output_file}")


if __name__ == "__main__":
    main()
```

---

## æ—¶é—´ä¼°ç®—

| ä»»åŠ¡è§„æ¨¡ | ä¼°è®¡æ—¶é—´ (æœ¬åœ° vLLM) | ä¼°è®¡æ—¶é—´ (API) |
|---------|---------------------|---------------|
| 10 tasks | 5-10 åˆ†é’Ÿ | 3-5 åˆ†é’Ÿ |
| 100 tasks | 1-2 å°æ—¶ | 30-60 åˆ†é’Ÿ |
| 500 tasks | 5-10 å°æ—¶ | 3-5 å°æ—¶ |
| 1000 tasks | 10-20 å°æ—¶ | 6-10 å°æ—¶ |

**æ³¨æ„**: 
- Crypto trading scenarios æ¯”è¾ƒè€—æ—¶ï¼ˆæ¯ä¸ª scenario æœ‰å¤šè½®äº¤äº’ï¼‰
- ä½¿ç”¨ `decision_interval: 5` å¯ä»¥å‡å°‘ crypto è¯„æµ‹æ—¶é—´ï¼ˆæ¯ 5 æ­¥å†³ç­–ä¸€æ¬¡ï¼‰
- GDPVal éœ€è¦ä¸‹è½½ HuggingFace æ•°æ®é›†ï¼Œé¦–æ¬¡è¿è¡Œè¾ƒæ…¢

---

## æ¨èçš„æŠ½æ ·ç­–ç•¥

å¯¹äºä»£è¡¨æ€§è¯„æµ‹ï¼Œå»ºè®®ï¼š

1. **å¿«é€ŸéªŒè¯** (10-20 tasks): æ¯ä¸ª dataset 2-3 ä¸ªæ ·æœ¬
2. **æ ‡å‡†è¯„æµ‹** (100 tasks): stratified æŠ½æ ·ï¼Œç¡®ä¿è¦†ç›–æ‰€æœ‰ task types
3. **å®Œæ•´è¯„æµ‹** (500+ tasks): åŒ…å«å…¨éƒ¨ crypto scenarios å’Œè¾ƒå¤§çš„ BizFinBench æ ·æœ¬

```yaml
# æ¨èçš„ä»£è¡¨æ€§æŠ½æ ·é…ç½®
sampling:
  strategy: stratified  # åˆ†å±‚æŠ½æ ·ç¡®ä¿å„ç±»ä»»åŠ¡å‡è¡¡
  total_limit: 100
  seed: 42              # å›ºå®šéšæœºç§å­ä¿è¯å¯é‡å¤æ€§
```

---

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•åˆ‡æ¢ä¸åŒçš„ LLM æ¨¡å‹ï¼Ÿ

ä¿®æ”¹ `.env` æ–‡ä»¶ä¸­çš„ `LLM_MODEL` å’Œç›¸å…³ API é…ç½®ï¼Œç„¶åé‡å¯ Purple Agentã€‚

### Q: Crypto æ•°æ®æ”¾åœ¨å“ªé‡Œï¼Ÿ

ä½¿ç”¨ `agentbusters-eval-data/crypto/eval_hidden` ç›®å½•ï¼Œåœ¨ eval_config.yaml ä¸­é…ç½®è·¯å¾„ã€‚

### Q: å¦‚ä½•ä¿è¯ç»“æœå¯é‡å¤ï¼Ÿ

1. è®¾ç½® `PURPLE_LLM_TEMPERATURE=0.0`
2. åœ¨ eval_config.yaml ä¸­è®¾ç½® `llm_eval.temperature: 0.0`
3. ä½¿ç”¨å›ºå®šçš„ `sampling.seed`
4. è®¾ç½® `shuffle: false`

### Q: å¦‚ä½•å¹¶è¡Œè¿è¡Œå¤šä¸ªå®éªŒï¼Ÿ

ä¸å»ºè®®åœ¨åŒä¸€æœºå™¨ä¸Šå¹¶è¡Œè¿è¡Œï¼Œå› ä¸ºèµ„æºç«äº‰å¯èƒ½å¯¼è‡´ç»“æœä¸ç¨³å®šã€‚å»ºè®®é¡ºåºè¿è¡Œæˆ–ä½¿ç”¨å¤šå°æœºå™¨ã€‚

### Q: vLLM æŠ¥é”™ "ImportError: libtorch_cuda.so: cannot open shared object file"ï¼Ÿ

è¿™æ˜¯å› ä¸ºå®‰è£…äº† CPU ç‰ˆæœ¬çš„ PyTorchã€‚**æœ€ä½³è§£å†³æ–¹æ³•æ˜¯è®© vLLM è‡ªåŠ¨ç®¡ç† PyTorchï¼š**

```bash
# æ¨èæ–¹æ³•ï¼šè®© vLLM è‡ªåŠ¨å¤„ç†
pip uninstall torch torchvision torchaudio vllm -y
pip install vllm  # vLLM ä¼šè‡ªåŠ¨å®‰è£…æ­£ç¡®çš„ PyTorch 2.9.x + CUDA
pip install "numpy<2.0"  # ä¿®å¤ NumPy å…¼å®¹æ€§
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
```

**å¦‚æœä¸Šè¿°æ–¹æ³•ä¸è¡Œï¼Œæ‰‹åŠ¨æŒ‡å®šç‰ˆæœ¬ï¼š**

```bash
# æ‰‹åŠ¨æ–¹æ³• (é¿å…ä½¿ç”¨ CUDA ç´¢å¼• URLï¼Œå®ƒä»¬ä¼šå®‰è£…é”™è¯¯ç‰ˆæœ¬)
pip uninstall torch torchvision torchaudio vllm -y
pip install torch==2.9.1 torchvision torchaudio  # ä¸ç”¨ç´¢å¼• URL
pip install vllm
pip install "numpy<2.0"
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
```

âš ï¸ **æ³¨æ„**: ä¸è¦ä½¿ç”¨ `--index-url` å› ä¸ºä¸åŒç´¢å¼•ä¼šå®‰è£…é”™è¯¯çš„ PyTorch ç‰ˆæœ¬ (cu126â†’2.10, cu124â†’2.4)ã€‚

### Q: vLLM æŠ¥é”™ "undefined symbol: _ZN3c104cuda..." æˆ–ç±»ä¼¼ C++ ç¬¦å·é”™è¯¯ï¼Ÿ

è¿™æ˜¯å› ä¸º vLLM å’Œ PyTorch çš„ CUDA ç‰ˆæœ¬ä¸åŒ¹é…ã€‚**æ¨èä½¿ç”¨ vLLM è‡ªåŠ¨ç®¡ç†æ–¹æ³•ï¼š**

```bash
# æœ€ä½³æ–¹æ³•ï¼šè®© vLLM é‡æ–°ç®¡ç† PyTorch ç‰ˆæœ¬
pip uninstall vllm torch torchvision torchaudio -y
pip install vllm --no-cache-dir  # ä¼šå®‰è£…æ­£ç¡®çš„ PyTorch 2.9.x
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
python -c "import vllm; print('vLLM working')"
```

### Q: ä¸ºä»€ä¹ˆä¸åŒçš„ CUDA ç´¢å¼•å®‰è£…ä¸åŒçš„ PyTorch ç‰ˆæœ¬ï¼Ÿ

PyTorch çš„ CUDA ç´¢å¼•ä¼šå®‰è£…ç‰¹å®šç‰ˆæœ¬ï¼š
- `cu126` â†’ PyTorch 2.10.x (å¯èƒ½ä¸ vLLM 0.15.0 ä¸å…¼å®¹)
- `cu124` â†’ PyTorch 2.4.x (å¤ªè€äº†)
- `cu121` â†’ PyTorch 2.1.x (å¤ªè€äº†)

**è§£å†³æ–¹æ³•ï¼š**
- æ–¹æ³•1ï¼ˆæ¨èï¼‰ï¼š`pip install vllm` è®© vLLM è‡ªåŠ¨é€‰æ‹© PyTorch 2.9.x
- æ–¹æ³•2ï¼šæ‰‹åŠ¨æŒ‡å®š `pip install torch==2.9.1` ä¸ä½¿ç”¨ç´¢å¼•URL

### Q: vLLM æŠ¥é”™ "auto tool choice requires --enable-auto-tool-choice"ï¼Ÿ

è¿™æ˜¯å› ä¸º Purple Agent ä½¿ç”¨äº†è‡ªåŠ¨å·¥å…·é€‰æ‹©åŠŸèƒ½ï¼Œä½† vLLM æœåŠ¡æ²¡æœ‰å¯ç”¨ç›¸å…³å‚æ•°ï¼š

```bash
# é”™è¯¯çš„å¯åŠ¨å‘½ä»¤ (ç¼ºå°‘å·¥å…·è°ƒç”¨æ”¯æŒ)
vllm serve Qwen/Qwen3-32B --port 8100 --tensor-parallel-size 2

# æ­£ç¡®çš„å¯åŠ¨å‘½ä»¤ (æ·»åŠ å·¥å…·è°ƒç”¨æ”¯æŒ)
vllm serve Qwen/Qwen3-32B --port 8100 --tensor-parallel-size 2 \
    --enable-auto-tool-choice --tool-call-parser qwen3_xml
```

**ä¸åŒæ¨¡å‹çš„å·¥å…·è§£æå™¨ (vLLM 0.15.0+):**
- Qwen3 ç³»åˆ—ï¼š`--tool-call-parser qwen3_xml`  
- DeepSeek-V3ï¼š`--tool-call-parser deepseek_v3`
- Llama3/4 ç³»åˆ—ï¼š`--tool-call-parser llama3_json` æˆ– `llama4_json`
- Mistral ç³»åˆ—ï¼š`--tool-call-parser mistral`

æŸ¥çœ‹æ‰€æœ‰å¯ç”¨è§£æå™¨ï¼š`vllm serve --help | grep tool-call-parser`

### Q: å‡ºç° NumPy å…¼å®¹æ€§è­¦å‘Šï¼Ÿ

å¦‚æœçœ‹åˆ° "A module that was compiled using NumPy 1.x cannot be run in NumPy 2.2.6" è­¦å‘Šï¼š

```bash
# é™çº§ NumPy åˆ° 1.x ç‰ˆæœ¬
pip install "numpy<2.0"

# éªŒè¯ä¿®å¤
python -c "import torch; import vllm; print('All packages working')"
```

### Q: vLLM æŠ¥é”™ "CUDA out of memory" å¦‚ä½•è§£å†³ï¼Ÿ

**åŸå› **: æ¨¡å‹å¤ªå¤§ï¼Œè¶…å‡º GPU å†…å­˜å®¹é‡ã€‚

**è§£å†³æ–¹æ¡ˆ (æŒ‰ä¼˜å…ˆçº§æ’åº):**

```bash
# æ–¹æ¡ˆ 1: é™ä½ GPU å†…å­˜ä½¿ç”¨ç‡
vllm serve Qwen/Qwen3-32B \
    --gpu-memory-utilization 0.80  # ä» 0.90 é™åˆ° 0.80

# æ–¹æ¡ˆ 2: ä½¿ç”¨æ›´å°çš„æ¨¡å‹
vllm serve Qwen/Qwen3-32B \
    --port 8000 \
    --tensor-parallel-size 1 \
    --max-model-len 32768 \
    --gpu-memory-utilization 0.90

# æ–¹æ¡ˆ 3: ä½¿ç”¨é‡åŒ– (å¦‚æœæ”¯æŒ)
vllm serve model_name \
    --quantization gptq  # æˆ– awq, fp8

# æ–¹æ¡ˆ 4: å¤š GPU å¹¶è¡Œ (å¦‚æœæœ‰å¤šå¼  GPU)
vllm serve large_model \
    --tensor-parallel-size 2  # ä½¿ç”¨ 2 å¼  GPU

# æ–¹æ¡ˆ 5: å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦
vllm serve model_name \
    --max-model-len 16384  # ä» 40960 å‡å°‘åˆ° 16384
```

**GH200-96GB æ¨èé…ç½®:**
- âœ… Qwen3-32B: æœ€ä½³å¹³è¡¡
- âœ… Qwen3-14B: è½»é‡çº§é€‰æ‹©
- âŒ Qwen3-235B-A22B: éœ€è¦ 480GB æˆ–å¤š GPU
- âŒ DeepSeek-V3: éœ€è¦å¤š GPU

### Q: DeepSeek-V3 æŠ¥é”™ "No valid attention backend found" / "FlashMLA Dense is only supported on Hopper devices"ï¼Ÿ

**å®Œæ•´é”™è¯¯ä¿¡æ¯:**
```
ValueError: No valid attention backend found for cuda with AttentionSelectorConfig(...use_mla=True...)
Reasons: {
  FLASHMLA: [compute capability not supported, FlashMLA Dense is only supported on Hopper devices.], 
  TRITON_MLA: [kv_cache_dtype not supported],
  ...
}
```

**æ ¹æœ¬åŸå› **: DeepSeek-V3 ä½¿ç”¨ **MLA (Multi-head Latent Attention)** æ¶æ„ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ³¨æ„åŠ›æœºåˆ¶ï¼Œ**åªèƒ½åœ¨ Hopper æ¶æ„ GPU (H100/H200) ä¸Šè¿è¡Œ**ã€‚

| GPU | Compute Capability | æ”¯æŒ DeepSeek-V3? |
|-----|-------------------|------------------|
| A100 | 8.0 | âŒ ä¸æ”¯æŒ |
| A6000 | 8.6 | âŒ ä¸æ”¯æŒ |
| RTX 4090 | 8.9 | âŒ ä¸æ”¯æŒ |
| **H100** | **9.0** | âœ… æ”¯æŒ |
| **H200** | **9.0** | âœ… æ”¯æŒ |
| **GH200** | **9.0** | âœ… æ”¯æŒ |

**è§£å†³æ–¹æ¡ˆ:**

1. **å°è¯•ç§»é™¤ FP8 KV cache** (å¯èƒ½è®© TRITON_MLA å·¥ä½œ):
```bash
# ç§»é™¤ --kv-cache-dtype fp8_e4m3 å‚æ•°
vllm serve deepseek-ai/DeepSeek-V3 \
    --port 8000 \
    --tensor-parallel-size 8 \
    --max-model-len 32768 \
    --gpu-memory-utilization 0.95 \
    --trust-remote-code \
    --quantization fp8 \
    --enable-auto-tool-choice \
    --tool-call-parser deepseek_v3
```

2. **ä½¿ç”¨æ›¿ä»£æ¨¡å‹** (å¦‚æœæ–¹æ¡ˆ1å¤±è´¥ - A100 æ¨è):
```bash
# Qwen3-32B - æ¨è
vllm serve Qwen/Qwen3-32B \
    --port 8000 \
    --tensor-parallel-size 2 \
    --max-model-len 32768 \
    --enable-auto-tool-choice \
    --tool-call-parser qwen3_xml

# Llama-3.1-70B
vllm serve meta-llama/Llama-3.1-70B-Instruct \
    --port 8000 \
    --tensor-parallel-size 2 \
    --enable-auto-tool-choice \
    --tool-call-parser llama3_json

# Mixtral-8x22B (MoE ä½†æ—  MLA é™åˆ¶)
vllm serve mistralai/Mixtral-8x22B-Instruct-v0.1 \
    --port 8000 \
    --tensor-parallel-size 4 \
    --enable-auto-tool-choice \
    --tool-call-parser mistral
```

3. **ä½¿ç”¨ API æœåŠ¡** (æ— éœ€æœ¬åœ° GPU):
```bash
# ä½¿ç”¨ OpenRouter API
export OPENAI_API_BASE=https://openrouter.ai/api/v1
export LLM_MODEL=deepseek/deepseek-chat
```

### Q: å¦‚ä½•æ£€æŸ¥ GPU å†…å­˜ä½¿ç”¨æƒ…å†µï¼Ÿ

```bash
# æŸ¥çœ‹ GPU çŠ¶æ€
nvidia-smi

# æŒç»­ç›‘æ§
watch -n 1 nvidia-smi

# åœ¨ Python ä¸­æ£€æŸ¥
python -c "import torch; print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')"
```

### Q: è¯„æµ‹æ—¶å‡ºç° "max_tokens is too large" æˆ–ä¸Šä¸‹æ–‡é•¿åº¦è¶…é™é”™è¯¯ï¼Ÿ

**é”™è¯¯ç¤ºä¾‹:**
```
llm_bizfinbench_failed: Error code: 400 - {'error': {'message': "'max_tokens' is too large: 800. 
This model's maximum context length is 32768 tokens and your request has 32180 input tokens..."}}
```

**åŸå› **: è¯„æµ‹ä½¿ç”¨çš„ LLM-as-judge æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦ä¸è¶³ä»¥å®¹çº³é•¿è¾“å…¥ + è¯„åˆ†è¾“å‡ºã€‚

**è§£å†³æ–¹æ¡ˆ:**

1. **ä½¿ç”¨æ›´å¤§ä¸Šä¸‹æ–‡çš„è¯„æµ‹æ¨¡å‹** (æ¨è):
```yaml
# config/eval_config.yaml
llm_eval:
  enabled: true
  model: gpt-4o-mini  # æ”¯æŒ 128K context
  temperature: 0.0
```

2. **å¢åŠ  vLLM ä¸Šä¸‹æ–‡é•¿åº¦** (å¦‚æœä½¿ç”¨æœ¬åœ°æ¨¡å‹è¯„æµ‹):
```bash
vllm serve Qwen/Qwen3-32B \
    --max-model-len 65536  # å¢åŠ åˆ° 64K
```

3. **ç³»ç»Ÿå·²è‡ªåŠ¨ä¼˜åŒ–**: è¯„æµ‹å™¨ä¼šè‡ªåŠ¨æˆªæ–­è¿‡é•¿è¾“å…¥å¹¶åŠ¨æ€è°ƒæ•´ max_tokens

### Q: è¯„æµ‹æ—¶å‡ºç° "LLM returned invalid JSON for PRBench evaluation"ï¼Ÿ

**åŸå› **: LLM æ²¡æœ‰è¿”å›æœ‰æ•ˆçš„ JSON æ ¼å¼å“åº”ï¼Œå¯èƒ½å› ä¸ºï¼š
- è¾“å‡ºè¢«æˆªæ–­
- æ¨¡å‹ä¸éµå¾ª JSON æ ¼å¼æŒ‡ä»¤
- ä¸Šä¸‹æ–‡æº¢å‡ºå¯¼è‡´å“åº”å¼‚å¸¸

**è§£å†³æ–¹æ¡ˆ:**

1. **ä½¿ç”¨éµå¾ªæŒ‡ä»¤èƒ½åŠ›æ›´å¼ºçš„æ¨¡å‹**:
```yaml
llm_eval:
  model: gpt-4o-mini  # æˆ– claude-3-haiku ç­‰
```

2. **æ£€æŸ¥ vLLM æ—¥å¿—** ç¡®è®¤æ¨¡å‹æ­£å¸¸å“åº”

3. **ç³»ç»Ÿå·²è‡ªåŠ¨å¤„ç†**: è¯„æµ‹å™¨ä¼šè‡ªåŠ¨é‡è¯•å¹¶ä½¿ç”¨ç®€åŒ–æç¤º

**æ³¨æ„**: è¿™äº›é”™è¯¯ä¸ä¼šå¯¼è‡´è¯„æµ‹å®Œå…¨å¤±è´¥ï¼Œåªæ˜¯è¯¥ä»»åŠ¡ä¼šä½¿ç”¨å¤‡ç”¨è¯„åˆ†ç­–ç•¥ï¼ˆè§„åˆ™åŒ¹é…ï¼‰ã€‚
