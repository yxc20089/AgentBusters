# AgentBusters Benchmark è¿è¡ŒæŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜å¦‚ä½•ä½¿ç”¨å®éªŒå®¤ GPU è¿è¡Œå¼€æº LLM è¿›è¡Œ benchmark æµ‹è¯•ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨ AgentBusters-Leaderboard æ”¶é›†ä¸åŒé…ç½®ä¸‹çš„æµ‹è¯•ç»“æœã€‚

## ç›®å½•

1. [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
2. [LLM é…ç½®é€‰é¡¹](#llm-é…ç½®é€‰é¡¹)
3. [è¯„æµ‹æ•°æ®é…ç½®](#è¯„æµ‹æ•°æ®é…ç½®)
4. [è¿è¡Œ Benchmark](#è¿è¡Œ-benchmark)
5. [å¤šé…ç½®å®éªŒç®¡ç†](#å¤šé…ç½®å®éªŒç®¡ç†)
6. [ç»“æœæ”¶é›†ä¸åˆ†æ](#ç»“æœæ”¶é›†ä¸åˆ†æ)

---

## ç¯å¢ƒå‡†å¤‡

### 1. åŸºç¡€å®‰è£…

```bash
# å…‹éš†ä»“åº“
cd d:\code\finbenchmark\AgentBusters

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv .venv
.\.venv\Scripts\Activate.ps1  # Windows PowerShell

# å®‰è£…ä¾èµ–
pip install -e ".[dev]"
```

### 2. é…ç½®æ–‡ä»¶è®¾ç½®

å¤åˆ¶ `.env.example` åˆ° `.env` å¹¶æ ¹æ®éœ€è¦ä¿®æ”¹ï¼š

```bash
cp .env.example .env
```

---

## LLM é…ç½®é€‰é¡¹

### Purple Agent LLM é…ç½®ï¼ˆåœ¨ `.env` ä¸­å®šä¹‰ï¼‰

Purple Agent æ˜¯è¢«è¯„æµ‹çš„é‡‘èåˆ†æ Agentï¼Œå…¶ LLM é…ç½®åœ¨ `.env` æ–‡ä»¶ä¸­è®¾ç½®ï¼š

```dotenv
# ============================================
# æ¨èé…ç½®: æœ¬åœ° vLLM éƒ¨ç½²ï¼ˆGPU æœåŠ¡å™¨ï¼‰
# ============================================
OPENAI_API_KEY=dummy                          # vLLM ä¸éœ€è¦çœŸå® API key
OPENAI_API_BASE=http://localhost:8000/v1      # vLLM æœåŠ¡åœ°å€
OPENAI_BASE_URL=http://localhost:8000/v1      # åˆ«å

# --- Qwen3-32Bï¼ˆæ¨èï¼Œå¹³è¡¡æ€§èƒ½ä¸èµ„æºï¼‰---
LLM_MODEL=Qwen/Qwen3-32B

# --- DeepSeek-V3.2ï¼ˆæœ€å¼ºæ€§èƒ½ï¼‰---
# LLM_MODEL=deepseek-ai/DeepSeek-V3

# --- Qwen3-14Bï¼ˆè½»é‡çº§ï¼‰---
# LLM_MODEL=Qwen/Qwen3-14B

# ============================================
# å¤‡é€‰: OpenRouter APIï¼ˆæ— éœ€æœ¬åœ° GPUï¼‰
# ============================================
# OPENAI_API_KEY=sk-or-v1-xxxxxxxxxxxxx
# OPENAI_API_BASE=https://openrouter.ai/api/v1
# LLM_MODEL=qwen/qwen3-32b              # Qwen3-32B via OpenRouter
# LLM_MODEL=deepseek/deepseek-chat      # DeepSeek via OpenRouter

# ============================================
# å•†ä¸š APIï¼ˆç”¨äºåŸºå‡†å¯¹æ¯”ï¼‰
# ============================================
# OPENAI_API_KEY=sk-...
# LLM_MODEL=gpt-4o

# LLM_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-...
# LLM_MODEL=claude-sonnet-4-20250514

# ============================================
# é€šç”¨é…ç½®
# ============================================
PURPLE_LLM_TEMPERATURE=0.0  # è®¾ä¸º 0.0 ä»¥è·å¾—å¯é‡å¤çš„ç»“æœ
```

### Green Agent LLM é…ç½®ï¼ˆåœ¨ eval_config.yaml ä¸­å®šä¹‰ï¼‰

Green Agent æ˜¯è¯„æµ‹å™¨ï¼Œä½¿ç”¨ LLM-as-judge è¿›è¡Œè¯„åˆ†ï¼š

```yaml
# config/eval_config.yaml
llm_eval:
  enabled: true
  model: gpt-4o-mini       # è¯„åˆ¤æ¨¡å‹
  temperature: 0.0         # å›ºå®šä¸º 0 ä»¥ä¿è¯å¯é‡å¤æ€§
```

### å¯åŠ¨æœ¬åœ° vLLM æœåŠ¡ï¼ˆGPU æœåŠ¡å™¨ï¼‰

```bash
# å®‰è£… vLLM
pip install vllm
```

#### æ¨èæ¨¡å‹éƒ¨ç½²

**âš¡ B200 é¡¶é… (å•å¡ 192GB HBM3e) - æœ€å¼ºå•å¡**

**1. Qwen3-235B-A22B (å•å¡ B200) - å•å¡è·‘ 235Bï¼**
```bash
# 1x B200 192GB - å•å¡è¿è¡Œ 235B MoE
vllm serve Qwen/Qwen3-235B-A22B \
    --port 8000 \
    --tensor-parallel-size 1 \
    --max-model-len 65536 \
    --gpu-memory-utilization 0.95 \
    --trust-remote-code

# å¿«é€Ÿéƒ¨ç½²å‘½ä»¤
python scripts/deploy_vllm.py --model qwen3-235b-b200
```

**2. DeepSeek-V3.2-671B (3x B200) - å…¨ç²¾åº¦ 671B**
```bash
# 3x B200 192GB - BF16 å…¨ç²¾åº¦è¿è¡Œ 671B MoE
vllm serve deepseek-ai/DeepSeek-V3 \
    --port 8000 \
    --tensor-parallel-size 3 \
    --max-model-len 65536 \
    --gpu-memory-utilization 0.95 \
    --trust-remote-code

# å¿«é€Ÿéƒ¨ç½²å‘½ä»¤
python scripts/deploy_vllm.py --model deepseek-v3-b200
```

---

**ğŸš€ H100 é¡¶é… (8x 80GB)**

**1. DeepSeek-V3.2-671B (FP8 åŸç”Ÿ) - H100 æœ€å¼ºæ€§èƒ½**
```bash
# 8x H100 80GB, FP8 åŸç”Ÿç²¾åº¦
vllm serve deepseek-ai/DeepSeek-V3 \
    --port 8000 \
    --tensor-parallel-size 8 \
    --max-model-len 32768 \
    --gpu-memory-utilization 0.95 \
    --trust-remote-code \
    --dtype float8_e4m3fn \
    --quantization fp8 \
    --kv-cache-dtype fp8_e4m3
```

**2. Qwen3-235B-A22B (MoE) - é¡¶çº§ MoE**
```bash
# 8x H100 80GB, 235B å‚æ•° (22B æ¿€æ´»)
vllm serve Qwen/Qwen3-235B-A22B \
    --port 8000 \
    --tensor-parallel-size 8 \
    --max-model-len 32768 \
    --gpu-memory-utilization 0.95 \
    --trust-remote-code
```

---

**ğŸ“Œ ä¸»è¦ç›®æ ‡æ¨¡å‹**

**1. Qwen3-32Bï¼ˆæ¨èï¼Œå¹³è¡¡æ€§èƒ½ä¸èµ„æºï¼‰**
```bash
# å• GPU (A100 80GB / H100 80GB)
vllm serve Qwen/Qwen3-32B \
    --port 8000 \
    --tensor-parallel-size 1 \
    --max-model-len 32768 \
    --gpu-memory-utilization 0.9

# åŒ GPU (2x A100 40GB / 2x RTX 4090)
vllm serve Qwen/Qwen3-32B \
    --port 8000 \
    --tensor-parallel-size 2 \
    --max-model-len 16384
```

**2. DeepSeek-V3.2ï¼ˆé«˜æ€§èƒ½ï¼Œéœ€è¦å¤š GPUï¼‰**
```bash
# 4x A100 80GB æˆ– 8x A100 40GB
vllm serve deepseek-ai/DeepSeek-V3 \
    --port 8000 \
    --tensor-parallel-size 4 \
    --max-model-len 16384 \
    --gpu-memory-utilization 0.95 \
    --trust-remote-code

# 8x GPU é…ç½®ï¼ˆæ›´å¤§ contextï¼‰
vllm serve deepseek-ai/DeepSeek-V3 \
    --port 8000 \
    --tensor-parallel-size 8 \
    --max-model-len 32768 \
    --trust-remote-code
```

**3. Qwen3-14Bï¼ˆè½»é‡çº§ï¼Œé€‚åˆå• GPUï¼‰**
```bash
# å• GPU (RTX 4090 24GB / A100 40GB)
vllm serve Qwen/Qwen3-14B \
    --port 8000 \
    --tensor-parallel-size 1 \
    --max-model-len 32768 \
    --gpu-memory-utilization 0.9

# RTX 3090 24GBï¼ˆå‡å°‘ context é•¿åº¦ï¼‰
vllm serve Qwen/Qwen3-14B \
    --port 8000 \
    --max-model-len 8192 \
    --gpu-memory-utilization 0.95
```

#### GPU å†…å­˜éœ€æ±‚å‚è€ƒ

| æ¨¡å‹ | å‚æ•°é‡ | æœ€ä½ GPU | æ¨èé…ç½® | Context é•¿åº¦ |
|------|--------|----------|----------|-------------|
| Qwen3-14B | 14B | 1x RTX 4090 (24GB) | 1x A100 40GB | 32K |
| Qwen3-32B | 32B | 2x RTX 4090 | 1x A100 80GB | 32K |
| DeepSeek-V3.2 | 671B MoE | 4x A100 80GB | 8x H100 80GB | 32K |
| Qwen3-235B-A22B | 235B MoE (22Bæ¿€æ´») | 8x H100 80GB | 8x H100 80GB | 32K |
| DeepSeek-V3 FP8 | 671B MoE | 8x H100 80GB | 8x H100 80GB | 32K |
| **âš¡ Qwen3-235B (B200)** | **235B MoE** | **1x B200 192GB** | **1x B200** | **65K** |
| **âš¡ DeepSeek-V3 (B200)** | **671B MoE** | **3x B200 192GB** | **3x B200** | **65K** |

#### å…¶ä»–æ¨¡å‹ï¼ˆå¤‡é€‰ï¼‰
```bash
# Llama 3.1 70B
vllm serve meta-llama/Llama-3.1-70B-Instruct \
    --port 8000 \
    --tensor-parallel-size 2

# Mixtral 8x22B
vllm serve mistralai/Mixtral-8x22B-Instruct-v0.1 \
    --port 8000 \
    --tensor-parallel-size 2
```

---

## è¯„æµ‹æ•°æ®é…ç½®

### è¯„æµ‹é…ç½®æ–‡ä»¶ç»“æ„

è¯„æµ‹é…ç½®åœ¨ `config/` ç›®å½•ä¸‹çš„ YAML æ–‡ä»¶ä¸­å®šä¹‰ã€‚ä»¥ä¸‹æ˜¯ä¸åŒè§„æ¨¡çš„é…ç½®ç¤ºä¾‹ï¼š

### å¿«é€Ÿæµ‹è¯•é…ç½®ï¼ˆ~10 tasksï¼‰

åˆ›å»º `config/eval_quick_test.yaml`:

```yaml
name: "Quick Test (10 tasks)"
version: "1.0"

datasets:
  - type: bizfinbench
    task_types: [event_logic_reasoning]
    languages: [en]
    limit_per_task: 3
    shuffle: false
    weight: 1.0

  - type: synthetic
    path: data/synthetic_questions/questions.json
    limit: 4
    shuffle: false
    weight: 1.0

  - type: options
    path: data/options/questions.json
    limit: 3
    shuffle: false
    weight: 1.0

sampling:
  strategy: stratified
  total_limit: 10
  seed: 42

llm_eval:
  enabled: true
  model: gpt-4o-mini
  temperature: 0.0

timeout_seconds: 300
```

### ä¸­ç­‰è§„æ¨¡é…ç½®ï¼ˆ~100 tasksï¼‰

åˆ›å»º `config/eval_medium.yaml`:

```yaml
name: "Medium Scale Evaluation (100 tasks)"
version: "1.0"

datasets:
  - type: bizfinbench
    task_types:
      - event_logic_reasoning
      - financial_quantitative_computation
      - anomaly_information_tracing
    languages: [en, cn]
    limit_per_task: 8
    shuffle: false
    weight: 1.0

  - type: prbench
    splits: [finance, legal]
    limit: 20
    shuffle: false
    weight: 1.0

  - type: synthetic
    path: data/synthetic_questions/questions.json
    limit: 20
    shuffle: false
    weight: 1.0

  - type: options
    path: data/options/questions.json
    limit: 20
    shuffle: false
    weight: 1.0

  - type: crypto
    path: ../agentbusters-eval-data/crypto/eval_hidden  # ä½¿ç”¨ eval-data ä¸­çš„æ•°æ®
    download_on_missing: false
    limit: 6
    shuffle: false
    weight: 1.0
    stride: 1
    max_steps: 100
    evaluation:
      initial_balance: 10000.0
      max_leverage: 3.0
      trading_fee: 0.0004
      price_noise_level: 0.001
      slippage_range: [0.0002, 0.0010]
      adversarial_injection_rate: 0.05
      decision_interval: 5
      funding_interval_hours: 8.0
      score_weights:
        baseline: 0.40
        noisy: 0.30
        adversarial: 0.20
        meta: 0.10
      metric_weights:
        sharpe: 0.50
        total_return: 0.25
        max_drawdown: 0.15
        win_rate: 0.10

  - type: gdpval
    hf_dataset: "openai/gdpval"
    limit: 10
    shuffle: false
    weight: 1.0

sampling:
  strategy: stratified
  total_limit: 100
  seed: 42

llm_eval:
  enabled: true
  model: gpt-4o-mini
  temperature: 0.0

timeout_seconds: 600
```

### å¤§è§„æ¨¡é…ç½®ï¼ˆ~1000 tasksï¼‰

åˆ›å»º `config/eval_large.yaml`:

```yaml
name: "Large Scale Evaluation (1000 tasks)"
version: "1.0"

datasets:
  - type: bizfinbench
    task_types:
      - anomaly_information_tracing
      - event_logic_reasoning
      - financial_data_description
      - financial_quantitative_computation
      - user_sentiment_analysis
      - stock_price_predict
      - financial_multi_turn_perception
    languages: [en, cn]
    limit_per_task: 50   # 7 types Ã— 2 languages Ã— 50 = 700
    shuffle: true
    weight: 1.0

  - type: prbench
    splits: [finance, legal, finance_hard, legal_hard]
    limit: 100
    shuffle: true
    weight: 1.0

  - type: synthetic
    path: data/synthetic_questions/questions.json
    limit: 50
    shuffle: true
    weight: 1.0

  - type: options
    path: data/options/questions.json
    limit: 50
    shuffle: true
    weight: 1.0

  - type: crypto
    path: ../agentbusters-eval-data/crypto/eval_hidden
    download_on_missing: false
    limit: 12  # å…¨éƒ¨ 12 ä¸ª scenarios
    shuffle: false
    weight: 1.0
    stride: 1
    max_steps: 200
    evaluation:
      initial_balance: 10000.0
      max_leverage: 3.0
      trading_fee: 0.0004
      price_noise_level: 0.001
      slippage_range: [0.0002, 0.0010]
      adversarial_injection_rate: 0.05
      decision_interval: 1
      funding_interval_hours: 8.0
      score_weights:
        baseline: 0.40
        noisy: 0.30
        adversarial: 0.20
        meta: 0.10
      metric_weights:
        sharpe: 0.50
        total_return: 0.25
        max_drawdown: 0.15
        win_rate: 0.10
      meta_transforms:
        - identity
        - scale_1_1
        - invert_returns

  - type: gdpval
    hf_dataset: "openai/gdpval"
    limit: 50
    shuffle: true
    weight: 1.0
    include_reference_files: true

sampling:
  strategy: stratified
  total_limit: 1000
  seed: 42

llm_eval:
  enabled: true
  model: gpt-4o-mini
  temperature: 0.0

timeout_seconds: 900
```

### ä½¿ç”¨ agentbusters-eval-data ä¸­çš„ Crypto æ•°æ®

ç¡®ä¿ crypto æ•°æ®è·¯å¾„æ­£ç¡®æŒ‡å‘ `agentbusters-eval-data`:

```yaml
- type: crypto
  path: ../agentbusters-eval-data/crypto/eval_hidden  # ç›¸å¯¹è·¯å¾„
  # æˆ–ä½¿ç”¨ç»å¯¹è·¯å¾„
  # path: d:/code/finbenchmark/agentbusters-eval-data/crypto/eval_hidden
```

å¯ç”¨çš„ crypto scenarios (å…± 12 ä¸ª):
- `scenario_520d87ed7569f147` (BTCUSDT)
- `scenario_b8aba67d7bfcc3b4` (BTCUSDT)
- `scenario_0a9c24d037aaa15c` (BTCUSDT)
- `scenario_9a1f49ebc9fcc664` (ETHUSDT)
- `scenario_a9d7b02930d276f2` (ETHUSDT)
- ... ç­‰

---

## è¿è¡Œ Benchmark

### æ–¹æ³• 1: æœ¬åœ°è¿è¡Œï¼ˆæ¨èç”¨äºå¼€å‘å’Œè°ƒè¯•ï¼‰

```bash
# ç»ˆç«¯ 1: å¯åŠ¨ Green Agent (è¯„æµ‹å™¨)
python src/cio_agent/a2a_server.py \
    --host 0.0.0.0 \
    --port 9109 \
    --eval-config config/eval_medium.yaml \
    --store-predicted \
    --predicted-max-chars 200

# ç»ˆç«¯ 2: å¯åŠ¨ Purple Agent (è¢«è¯„æµ‹çš„ Agent)
purple-agent serve --host 0.0.0.0 --port 9110 --card-url http://127.0.0.1:9110

# ç»ˆç«¯ 3: è¿è¡Œè¯„æµ‹
python scripts/run_a2a_eval.py \
    --green-url http://127.0.0.1:9109 \
    --purple-url http://127.0.0.1:9110 \
    --num-tasks 100 \
    --timeout 1800 \
    -v \
    -o results/eval_medium_$(date +%Y%m%d_%H%M%S).json
```

### æ–¹æ³• 2: Docker è¿è¡Œ

```bash
# æ„å»ºé•œåƒ
docker build -t agentbusters-green -f Dockerfile.green .
docker build -t agentbusters-purple -f Dockerfile.purple .

# è¿è¡Œ
docker-compose up
```

### æ–¹æ³• 3: ä½¿ç”¨ Leaderboard æ¡†æ¶

å‚è§ä¸‹ä¸€èŠ‚ [å¤šé…ç½®å®éªŒç®¡ç†](#å¤šé…ç½®å®éªŒç®¡ç†)ã€‚

---

## å¤šé…ç½®å®éªŒç®¡ç†

ä½¿ç”¨ AgentBusters-Leaderboard æ¡†æ¶æ¥ç³»ç»Ÿåœ°ç®¡ç†ä¸åŒé…ç½®ä¸‹çš„å®éªŒç»“æœã€‚

### å®éªŒé…ç½®æ¨¡æ¿

åˆ›å»º `experiments/experiment_configs.yaml`:

```yaml
# å®éªŒé…ç½®å®šä¹‰
experiments:
  # å®éªŒ 1: ä¸åŒæ¨¡å‹å¯¹æ¯”
  - name: "model_comparison"
    description: "Compare different LLM models"
    configs:
      - id: "llama3.1-70b"
        llm_model: "meta-llama/llama-3.1-70b-instruct"
        eval_config: "config/eval_medium.yaml"
        num_tasks: 100
        
      - id: "qwen2.5-72b"
        llm_model: "qwen/qwen-2.5-72b-instruct"
        eval_config: "config/eval_medium.yaml"
        num_tasks: 100
        
      - id: "deepseek-chat"
        llm_model: "deepseek/deepseek-chat"
        eval_config: "config/eval_medium.yaml"
        num_tasks: 100
        
      - id: "mixtral-8x22b"
        llm_model: "mistralai/mixtral-8x22b-instruct"
        eval_config: "config/eval_medium.yaml"
        num_tasks: 100

  # å®éªŒ 2: ä¸åŒä»»åŠ¡æ•°é‡å¯¹æ¯”
  - name: "scale_comparison"
    description: "Compare evaluation at different scales"
    configs:
      - id: "scale-10"
        llm_model: "meta-llama/llama-3.1-70b-instruct"
        eval_config: "config/eval_quick_test.yaml"
        num_tasks: 10
        
      - id: "scale-100"
        llm_model: "meta-llama/llama-3.1-70b-instruct"
        eval_config: "config/eval_medium.yaml"
        num_tasks: 100
        
      - id: "scale-500"
        llm_model: "meta-llama/llama-3.1-70b-instruct"
        eval_config: "config/eval_large.yaml"
        num_tasks: 500

  # å®éªŒ 3: æŠ½æ ·ç­–ç•¥å¯¹æ¯”
  - name: "sampling_comparison"
    description: "Compare different sampling strategies"
    configs:
      - id: "stratified"
        sampling_strategy: "stratified"
        num_tasks: 100
        
      - id: "random"
        sampling_strategy: "random"
        num_tasks: 100
        
      - id: "sequential"
        sampling_strategy: "sequential"
        num_tasks: 100
```

### æ‰¹é‡è¿è¡Œè„šæœ¬

åˆ›å»º `scripts/run_experiments.py`:

```python
#!/usr/bin/env python3
"""
æ‰¹é‡è¿è¡Œå¤šé…ç½®å®éªŒ

Usage:
    python scripts/run_experiments.py --experiment model_comparison
    python scripts/run_experiments.py --all
"""

import argparse
import json
import os
import subprocess
import sys
from datetime import datetime
from pathlib import Path

import yaml


def load_experiment_configs(config_path: str) -> dict:
    """åŠ è½½å®éªŒé…ç½®"""
    with open(config_path) as f:
        return yaml.safe_load(f)


def run_single_experiment(
    config_id: str,
    llm_model: str,
    eval_config: str,
    num_tasks: int,
    output_dir: str,
    green_url: str = "http://localhost:9109",
    purple_url: str = "http://localhost:9110",
    timeout: int = 1800,
) -> dict:
    """è¿è¡Œå•ä¸ªå®éªŒé…ç½®"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = Path(output_dir) / f"{config_id}_{timestamp}.json"
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    env = os.environ.copy()
    env["LLM_MODEL"] = llm_model
    
    # æ„å»ºå‘½ä»¤
    cmd = [
        sys.executable,
        "scripts/run_a2a_eval.py",
        "--green-url", green_url,
        "--purple-url", purple_url,
        "--num-tasks", str(num_tasks),
        "--timeout", str(timeout),
        "-v",
        "-o", str(output_file),
    ]
    
    print(f"\n{'='*60}")
    print(f"Running experiment: {config_id}")
    print(f"  Model: {llm_model}")
    print(f"  Tasks: {num_tasks}")
    print(f"  Output: {output_file}")
    print(f"{'='*60}\n")
    
    result = subprocess.run(cmd, env=env, capture_output=False)
    
    return {
        "config_id": config_id,
        "llm_model": llm_model,
        "num_tasks": num_tasks,
        "output_file": str(output_file),
        "success": result.returncode == 0,
        "timestamp": timestamp,
    }


def run_experiment_suite(
    experiment_name: str,
    configs: list,
    output_dir: str,
) -> list:
    """è¿è¡Œä¸€ç»„å®éªŒ"""
    
    results = []
    for config in configs:
        result = run_single_experiment(
            config_id=config["id"],
            llm_model=config.get("llm_model", os.getenv("LLM_MODEL", "gpt-4o")),
            eval_config=config.get("eval_config", "config/eval_config.yaml"),
            num_tasks=config.get("num_tasks", 100),
            output_dir=output_dir,
        )
        results.append(result)
    
    return results


def main():
    parser = argparse.ArgumentParser(description="Run multiple experiment configurations")
    parser.add_argument("--config", default="experiments/experiment_configs.yaml")
    parser.add_argument("--experiment", help="Specific experiment to run")
    parser.add_argument("--all", action="store_true", help="Run all experiments")
    parser.add_argument("--output-dir", default="results/experiments")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½é…ç½®
    config = load_experiment_configs(args.config)
    
    all_results = []
    
    for experiment in config["experiments"]:
        if args.all or args.experiment == experiment["name"]:
            print(f"\n{'#'*60}")
            print(f"# Experiment: {experiment['name']}")
            print(f"# {experiment['description']}")
            print(f"{'#'*60}")
            
            results = run_experiment_suite(
                experiment["name"],
                experiment["configs"],
                args.output_dir,
            )
            all_results.extend(results)
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    summary_file = Path(args.output_dir) / "experiment_summary.json"
    with open(summary_file, "w") as f:
        json.dump(all_results, f, indent=2)
    
    print(f"\nâœ… Experiment summary saved to: {summary_file}")


if __name__ == "__main__":
    main()
```

### Leaderboard ç»“æœæ”¶é›†

ä¿®æ”¹ `AgentBusters-Leaderboard/scenario.toml` æ¥å®šä¹‰ä¸åŒçš„å®éªŒ:

```toml
# scenario.toml - å¤šé…ç½®å®éªŒç¤ºä¾‹

[green_agent]
agentbeats_id = "019bc421-99d0-7ee3-ae27-658145eff474"
env = { 
    OPENAI_API_KEY = "${OPENAI_API_KEY}", 
    EVAL_CONFIG = "config/eval_medium.yaml",  # å¯ä¿®æ”¹ä¸ºä¸åŒçš„é…ç½®
    EVAL_DATA_REPO = "${EVAL_DATA_REPO}", 
    EVAL_DATA_PAT = "${EVAL_DATA_PAT}" 
}

[[participants]]
agentbeats_id = "019c16a8-a0b2-77c3-aae3-8e0c23ca5de1"
name = "purple_agent"
env = { 
    OPENAI_API_KEY = "${OPENAI_API_KEY}",
    OPENAI_API_BASE = "https://openrouter.ai/api/v1",  # æˆ–æœ¬åœ° vLLM
    LLM_MODEL = "meta-llama/llama-3.1-70b-instruct"   # è¦æµ‹è¯•çš„æ¨¡å‹
}

[config]
num_tasks = 100              # ä»»åŠ¡æ•°é‡
conduct_debate = false
timeout_seconds = 600
datasets = ["bizfinbench", "synthetic", "options", "crypto", "gdpval"]
sampling_strategy = "stratified"

# æ•°æ®é›†é™åˆ¶
bizfinbench_limit = 30
synthetic_limit = 20
options_limit = 20
crypto_limit = 12
gdpval_limit = 18
```

---

## ç»“æœæ”¶é›†ä¸åˆ†æ

### ç»“æœæ–‡ä»¶æ ¼å¼

æ¯æ¬¡è¿è¡Œä¼šç”Ÿæˆ JSON æ ¼å¼çš„ç»“æœæ–‡ä»¶ï¼š

```json
{
  "timestamp": "2026-02-02T10:30:00Z",
  "config": {
    "llm_model": "meta-llama/llama-3.1-70b-instruct",
    "num_tasks": 100,
    "eval_config": "config/eval_medium.yaml"
  },
  "results": {
    "overall_score": 65.4,
    "section_scores": {
      "knowledge": 70.2,
      "analysis": 62.5,
      "options": 58.3,
      "crypto": 71.8
    },
    "dataset_scores": {
      "bizfinbench": 0.72,
      "synthetic": 0.58,
      "options": 0.55,
      "crypto": 0.68,
      "gdpval": 0.61
    }
  }
}
```

### ç»“æœæ±‡æ€»è„šæœ¬

åˆ›å»º `scripts/aggregate_results.py`:

```python
#!/usr/bin/env python3
"""æ±‡æ€»å¤šæ¬¡å®éªŒç»“æœ"""

import json
import sys
from pathlib import Path
import pandas as pd


def load_results(results_dir: str) -> list:
    """åŠ è½½æ‰€æœ‰ç»“æœæ–‡ä»¶"""
    results = []
    for file in Path(results_dir).glob("*.json"):
        if file.name == "experiment_summary.json":
            continue
        with open(file) as f:
            data = json.load(f)
            data["filename"] = file.name
            results.append(data)
    return results


def create_summary_table(results: list) -> pd.DataFrame:
    """åˆ›å»ºæ±‡æ€»è¡¨æ ¼"""
    rows = []
    for r in results:
        config = r.get("config", {})
        scores = r.get("results", {})
        rows.append({
            "Model": config.get("llm_model", "unknown"),
            "Tasks": config.get("num_tasks", 0),
            "Overall": scores.get("overall_score", 0),
            "Knowledge": scores.get("section_scores", {}).get("knowledge", 0),
            "Analysis": scores.get("section_scores", {}).get("analysis", 0),
            "Options": scores.get("section_scores", {}).get("options", 0),
            "Crypto": scores.get("section_scores", {}).get("crypto", 0),
            "File": r.get("filename", ""),
        })
    
    df = pd.DataFrame(rows)
    df = df.sort_values("Overall", ascending=False)
    return df


def main():
    if len(sys.argv) < 2:
        print("Usage: python aggregate_results.py <results_dir>")
        sys.exit(1)
    
    results = load_results(sys.argv[1])
    df = create_summary_table(results)
    
    print("\n" + "="*80)
    print("BENCHMARK RESULTS SUMMARY")
    print("="*80)
    print(df.to_string(index=False))
    
    # ä¿å­˜ä¸º CSV
    output_file = Path(sys.argv[1]) / "leaderboard.csv"
    df.to_csv(output_file, index=False)
    print(f"\nâœ… Saved to: {output_file}")


if __name__ == "__main__":
    main()
```

---

## æ—¶é—´ä¼°ç®—

| ä»»åŠ¡è§„æ¨¡ | ä¼°è®¡æ—¶é—´ (æœ¬åœ° vLLM) | ä¼°è®¡æ—¶é—´ (API) |
|---------|---------------------|---------------|
| 10 tasks | 5-10 åˆ†é’Ÿ | 3-5 åˆ†é’Ÿ |
| 100 tasks | 1-2 å°æ—¶ | 30-60 åˆ†é’Ÿ |
| 500 tasks | 5-10 å°æ—¶ | 3-5 å°æ—¶ |
| 1000 tasks | 10-20 å°æ—¶ | 6-10 å°æ—¶ |

**æ³¨æ„**: 
- Crypto trading scenarios æ¯”è¾ƒè€—æ—¶ï¼ˆæ¯ä¸ª scenario æœ‰å¤šè½®äº¤äº’ï¼‰
- ä½¿ç”¨ `decision_interval: 5` å¯ä»¥å‡å°‘ crypto è¯„æµ‹æ—¶é—´ï¼ˆæ¯ 5 æ­¥å†³ç­–ä¸€æ¬¡ï¼‰
- GDPVal éœ€è¦ä¸‹è½½ HuggingFace æ•°æ®é›†ï¼Œé¦–æ¬¡è¿è¡Œè¾ƒæ…¢

---

## æ¨èçš„æŠ½æ ·ç­–ç•¥

å¯¹äºä»£è¡¨æ€§è¯„æµ‹ï¼Œå»ºè®®ï¼š

1. **å¿«é€ŸéªŒè¯** (10-20 tasks): æ¯ä¸ª dataset 2-3 ä¸ªæ ·æœ¬
2. **æ ‡å‡†è¯„æµ‹** (100 tasks): stratified æŠ½æ ·ï¼Œç¡®ä¿è¦†ç›–æ‰€æœ‰ task types
3. **å®Œæ•´è¯„æµ‹** (500+ tasks): åŒ…å«å…¨éƒ¨ crypto scenarios å’Œè¾ƒå¤§çš„ BizFinBench æ ·æœ¬

```yaml
# æ¨èçš„ä»£è¡¨æ€§æŠ½æ ·é…ç½®
sampling:
  strategy: stratified  # åˆ†å±‚æŠ½æ ·ç¡®ä¿å„ç±»ä»»åŠ¡å‡è¡¡
  total_limit: 100
  seed: 42              # å›ºå®šéšæœºç§å­ä¿è¯å¯é‡å¤æ€§
```

---

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•åˆ‡æ¢ä¸åŒçš„ LLM æ¨¡å‹ï¼Ÿ

ä¿®æ”¹ `.env` æ–‡ä»¶ä¸­çš„ `LLM_MODEL` å’Œç›¸å…³ API é…ç½®ï¼Œç„¶åé‡å¯ Purple Agentã€‚

### Q: Crypto æ•°æ®æ”¾åœ¨å“ªé‡Œï¼Ÿ

ä½¿ç”¨ `agentbusters-eval-data/crypto/eval_hidden` ç›®å½•ï¼Œåœ¨ eval_config.yaml ä¸­é…ç½®è·¯å¾„ã€‚

### Q: å¦‚ä½•ä¿è¯ç»“æœå¯é‡å¤ï¼Ÿ

1. è®¾ç½® `PURPLE_LLM_TEMPERATURE=0.0`
2. åœ¨ eval_config.yaml ä¸­è®¾ç½® `llm_eval.temperature: 0.0`
3. ä½¿ç”¨å›ºå®šçš„ `sampling.seed`
4. è®¾ç½® `shuffle: false`

### Q: å¦‚ä½•å¹¶è¡Œè¿è¡Œå¤šä¸ªå®éªŒï¼Ÿ

ä¸å»ºè®®åœ¨åŒä¸€æœºå™¨ä¸Šå¹¶è¡Œè¿è¡Œï¼Œå› ä¸ºèµ„æºç«äº‰å¯èƒ½å¯¼è‡´ç»“æœä¸ç¨³å®šã€‚å»ºè®®é¡ºåºè¿è¡Œæˆ–ä½¿ç”¨å¤šå°æœºå™¨ã€‚
